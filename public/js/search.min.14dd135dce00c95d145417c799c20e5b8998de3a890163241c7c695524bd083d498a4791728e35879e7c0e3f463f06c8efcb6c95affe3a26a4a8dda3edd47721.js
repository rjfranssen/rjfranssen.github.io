var e,t;e=this,t=function(){"use strict";function e(t){return(e="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&"function"==typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?"symbol":typeof e})(t)}function t(e,t){if(!(e instanceof t))throw new TypeError("Cannot call a class as a function")}function n(e,t){for(var n=0;n<t.length;n++){var r=t[n];r.enumerable=r.enumerable||!1,r.configurable=!0,"value"in r&&(r.writable=!0),Object.defineProperty(e,r.key,r)}}function r(e,t,r){return t&&n(e.prototype,t),r&&n(e,r),e}function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function c(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function a(e,t){if("function"!=typeof t&&null!==t)throw new TypeError("Super expression must either be null or a function");e.prototype=Object.create(t&&t.prototype,{constructor:{value:e,writable:!0,configurable:!0}}),t&&u(e,t)}function s(e){return(s=Object.setPrototypeOf?Object.getPrototypeOf:function(e){return e.__proto__||Object.getPrototypeOf(e)})(e)}function u(e,t){return(u=Object.setPrototypeOf||function(e,t){return e.__proto__=t,e})(e,t)}function h(e,t){return!t||"object"!=typeof t&&"function"!=typeof t?function(e){if(void 0===e)throw new ReferenceError("this hasn't been initialised - super() hasn't been called");return e}(e):t}function f(e){var t=function(){if("undefined"==typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if("function"==typeof Proxy)return!0;try{return Date.prototype.toString.call(Reflect.construct(Date,[],(function(){}))),!0}catch(e){return!1}}();return function(){var n,r=s(e);if(t){var i=s(this).constructor;n=Reflect.construct(r,arguments,i)}else n=r.apply(this,arguments);return h(this,n)}}function l(e){return function(e){if(Array.isArray(e))return d(e)}(e)||function(e){if("undefined"!=typeof Symbol&&Symbol.iterator in Object(e))return Array.from(e)}(e)||function(e,t){if(e){if("string"==typeof e)return d(e,t);var n=Object.prototype.toString.call(e).slice(8,-1);return"Object"===n&&e.constructor&&(n=e.constructor.name),"Map"===n||"Set"===n?Array.from(e):"Arguments"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n)?d(e,t):void 0}}(e)||function(){throw new TypeError("Invalid attempt to spread non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}()}function d(e,t){(null==t||t>e.length)&&(t=e.length);for(var n=0,r=new Array(t);n<t;n++)r[n]=e[n];return r}function v(e){return Array.isArray?Array.isArray(e):"[object Array]"===x(e)}function g(e){return"string"==typeof e}function y(e){return"number"==typeof e}function p(e){return!0===e||!1===e||function(e){return m(e)&&null!==e}(e)&&"[object Boolean]"==x(e)}function m(t){return"object"===e(t)}function k(e){return null!=e}function M(e){return!e.trim().length}function x(e){return null==e?void 0===e?"[object Undefined]":"[object Null]":Object.prototype.toString.call(e)}var b=function(e){return"Invalid value for key ".concat(e)},L=function(e){return"Pattern length exceeds max of ".concat(e,".")},S=Object.prototype.hasOwnProperty,_=function(){function e(n){var r=this;t(this,e),this._keys=[],this._keyMap={};var i=0;n.forEach((function(e){var t=w(e);i+=t.weight,r._keys.push(t),r._keyMap[t.id]=t,i+=t.weight})),this._keys.forEach((function(e){e.weight/=i}))}return r(e,[{key:"get",value:function(e){return this._keyMap[e]}},{key:"keys",value:function(){return this._keys}},{key:"toJSON",value:function(){return JSON.stringify(this._keys)}}]),e}();function w(e){var t=null,n=null,r=null,i=1;if(g(e)||v(e))r=e,t=O(e),n=j(e);else{if(!S.call(e,"name"))throw new Error(function(e){return"Missing ".concat(e," property in key")}("name"));var o=e.name;if(r=o,S.call(e,"weight")&&(i=e.weight)<=0)throw new Error(function(e){return"Property 'weight' in key '".concat(e,"' must be a positive integer")}(o));t=O(o),n=j(o)}return{path:t,id:n,weight:i,src:r}}function O(e){return v(e)?e:e.split(".")}function j(e){return v(e)?e.join("."):e}var A=c({},{isCaseSensitive:!1,includeScore:!1,keys:[],shouldSort:!0,sortFn:function(e,t){return e.score===t.score?e.idx<t.idx?-1:1:e.score<t.score?-1:1}},{},{includeMatches:!1,findAllMatches:!1,minMatchCharLength:1},{},{location:0,threshold:.6,distance:100},{},{useExtendedSearch:!1,getFn:function(e,t){var n=[],r=!1;return function e(t,i,o){if(k(t))if(i[o]){var c=t[i[o]];if(!k(c))return;if(o===i.length-1&&(g(c)||y(c)||p(c)))n.push(function(e){return null==e?"":function(e){if("string"==typeof e)return e;var t=e+"";return"0"==t&&1/e==-1/0?"-0":t}(e)}(c));else if(v(c)){r=!0;for(var a=0,s=c.length;a<s;a+=1)e(c[a],i,o+1)}else i.length&&e(c,i,o+1)}else n.push(t)}(e,g(t)?t.split("."):t,0),r?n:n[0]},ignoreLocation:!1,ignoreFieldNorm:!1}),I=/[^ ]+/g;function C(){var e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:3,t=new Map;return{get:function(n){var r=n.match(I).length;if(t.has(r))return t.get(r);var i=parseFloat((1/Math.sqrt(r)).toFixed(e));return t.set(r,i),i},clear:function(){t.clear()}}}var E=function(){function e(){var n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},r=n.getFn,i=void 0===r?A.getFn:r;t(this,e),this.norm=C(3),this.getFn=i,this.isCreated=!1,this.setIndexRecords()}return r(e,[{key:"setSources",value:function(){var e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:[];this.docs=e}},{key:"setIndexRecords",value:function(){var e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:[];this.records=e}},{key:"setKeys",value:function(){var e=this,t=arguments.length>0&&void 0!==arguments[0]?arguments[0]:[];this.keys=t,this._keysMap={},t.forEach((function(t,n){e._keysMap[t.id]=n}))}},{key:"create",value:function(){var e=this;!this.isCreated&&this.docs.length&&(this.isCreated=!0,g(this.docs[0])?this.docs.forEach((function(t,n){e._addString(t,n)})):this.docs.forEach((function(t,n){e._addObject(t,n)})),this.norm.clear())}},{key:"add",value:function(e){var t=this.size();g(e)?this._addString(e,t):this._addObject(e,t)}},{key:"removeAt",value:function(e){this.records.splice(e,1);for(var t=e,n=this.size();t<n;t+=1)this.records[t].i-=1}},{key:"getValueForItemAtKeyId",value:function(e,t){return e[this._keysMap[t]]}},{key:"size",value:function(){return this.records.length}},{key:"_addString",value:function(e,t){if(k(e)&&!M(e)){var n={v:e,i:t,n:this.norm.get(e)};this.records.push(n)}}},{key:"_addObject",value:function(e,t){var n=this,r={i:t,$:{}};this.keys.forEach((function(t,i){var o=n.getFn(e,t.path);if(k(o))if(v(o))!function(){for(var e=[],t=[{nestedArrIndex:-1,value:o}];t.length;){var c=t.pop(),a=c.nestedArrIndex,s=c.value;if(k(s))if(g(s)&&!M(s)){var u={v:s,i:a,n:n.norm.get(s)};e.push(u)}else v(s)&&s.forEach((function(e,n){t.push({nestedArrIndex:n,value:e})}))}r.$[i]=e}();else if(!M(o)){var c={v:o,n:n.norm.get(o)};r.$[i]=c}})),this.records.push(r)}},{key:"toJSON",value:function(){return{keys:this.keys,records:this.records}}}]),e}();function $(e,t){var n=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{},r=n.getFn,i=void 0===r?A.getFn:r,o=new E({getFn:i});return o.setKeys(e.map(w)),o.setSources(t),o.create(),o}function R(e,t){var n=e.matches;t.matches=[],k(n)&&n.forEach((function(e){if(k(e.indices)&&e.indices.length){var n={indices:e.indices,value:e.value};e.key&&(n.key=e.key.src),e.idx>-1&&(n.refIndex=e.idx),t.matches.push(n)}}))}function F(e,t){t.score=e.score}function P(e){var t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},n=t.errors,r=void 0===n?0:n,i=t.currentLocation,o=void 0===i?0:i,c=t.expectedLocation,a=void 0===c?0:c,s=t.distance,u=void 0===s?A.distance:s,h=t.ignoreLocation,f=void 0===h?A.ignoreLocation:h,l=r/e.length;if(f)return l;var d=Math.abs(a-o);return u?l+d/u:d?1:l}function N(){for(var e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:[],t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:A.minMatchCharLength,n=[],r=-1,i=-1,o=0,c=e.length;o<c;o+=1){var a=e[o];a&&-1===r?r=o:a||-1===r||((i=o-1)-r+1>=t&&n.push([r,i]),r=-1)}return e[o-1]&&o-r>=t&&n.push([r,o-1]),n}function D(e){for(var t={},n=0,r=e.length;n<r;n+=1){var i=e.charAt(n);t[i]=(t[i]||0)|1<<r-n-1}return t}var z=function(){function e(n){var r=this,i=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},o=i.location,c=void 0===o?A.location:o,a=i.threshold,s=void 0===a?A.threshold:a,u=i.distance,h=void 0===u?A.distance:u,f=i.includeMatches,l=void 0===f?A.includeMatches:f,d=i.findAllMatches,v=void 0===d?A.findAllMatches:d,g=i.minMatchCharLength,y=void 0===g?A.minMatchCharLength:g,p=i.isCaseSensitive,m=void 0===p?A.isCaseSensitive:p,k=i.ignoreLocation,M=void 0===k?A.ignoreLocation:k;if(t(this,e),this.options={location:c,threshold:s,distance:h,includeMatches:l,findAllMatches:v,minMatchCharLength:y,isCaseSensitive:m,ignoreLocation:M},this.pattern=m?n:n.toLowerCase(),this.chunks=[],this.pattern.length){var x=function(e,t){r.chunks.push({pattern:e,alphabet:D(e),startIndex:t})},b=this.pattern.length;if(b>32){for(var L=0,S=b%32,_=b-S;L<_;)x(this.pattern.substr(L,32),L),L+=32;if(S){var w=b-32;x(this.pattern.substr(w),w)}}else x(this.pattern,0)}}return r(e,[{key:"searchIn",value:function(e){var t=this.options,n=t.isCaseSensitive,r=t.includeMatches;if(n||(e=e.toLowerCase()),this.pattern===e){var i={isMatch:!0,score:0};return r&&(i.indices=[[0,e.length-1]]),i}var o=this.options,c=o.location,a=o.distance,s=o.threshold,u=o.findAllMatches,h=o.minMatchCharLength,f=o.ignoreLocation,d=[],v=0,g=!1;this.chunks.forEach((function(t){var n=t.pattern,i=t.alphabet,o=t.startIndex,y=function(e,t,n){var r=arguments.length>3&&void 0!==arguments[3]?arguments[3]:{},i=r.location,o=void 0===i?A.location:i,c=r.distance,a=void 0===c?A.distance:c,s=r.threshold,u=void 0===s?A.threshold:s,h=r.findAllMatches,f=void 0===h?A.findAllMatches:h,l=r.minMatchCharLength,d=void 0===l?A.minMatchCharLength:l,v=r.includeMatches,g=void 0===v?A.includeMatches:v,y=r.ignoreLocation,p=void 0===y?A.ignoreLocation:y;if(t.length>32)throw new Error(L(32));for(var m,k=t.length,M=e.length,x=Math.max(0,Math.min(o,M)),b=u,S=x,_=d>1||g,w=_?Array(M):[];(m=e.indexOf(t,S))>-1;){var O=P(t,{currentLocation:m,expectedLocation:x,distance:a,ignoreLocation:p});if(b=Math.min(O,b),S=m+k,_)for(var j=0;j<k;)w[m+j]=1,j+=1}S=-1;for(var I=[],C=1,E=k+M,$=1<<k-1,R=0;R<k;R+=1){for(var F=0,D=E;F<D;){var z=P(t,{errors:R,currentLocation:x+D,expectedLocation:x,distance:a,ignoreLocation:p});z<=b?F=D:E=D,D=Math.floor((E-F)/2+F)}E=D;var K=Math.max(1,x-D+1),q=f?M:Math.min(x+D,M)+k,W=Array(q+2);W[q+1]=(1<<R)-1;for(var J=q;J>=K;J-=1){var T=J-1,U=n[e.charAt(T)];if(_&&(w[T]=+!!U),W[J]=(W[J+1]<<1|1)&U,R&&(W[J]|=(I[J+1]|I[J])<<1|1|I[J+1]),W[J]&$&&(C=P(t,{errors:R,currentLocation:T,expectedLocation:x,distance:a,ignoreLocation:p}))<=b){if(b=C,(S=T)<=x)break;K=Math.max(1,2*x-S)}}var V=P(t,{errors:R+1,currentLocation:x,expectedLocation:x,distance:a,ignoreLocation:p});if(V>b)break;I=W}var B={isMatch:S>=0,score:Math.max(.001,C)};if(_){var G=N(w,d);G.length?g&&(B.indices=G):B.isMatch=!1}return B}(e,n,i,{location:c+o,distance:a,threshold:s,findAllMatches:u,minMatchCharLength:h,includeMatches:r,ignoreLocation:f}),p=y.isMatch,m=y.score,k=y.indices;p&&(g=!0),v+=m,p&&k&&(d=[].concat(l(d),l(k)))}));var y={isMatch:g,score:g?v/this.chunks.length:1};return g&&r&&(y.indices=d),y}}]),e}(),K=function(){function e(n){t(this,e),this.pattern=n}return r(e,[{key:"search",value:function(){}}],[{key:"isMultiMatch",value:function(e){return q(e,this.multiRegex)}},{key:"isSingleMatch",value:function(e){return q(e,this.singleRegex)}}]),e}();function q(e,t){var n=e.match(t);return n?n[1]:null}var W=function(e){a(i,e);var n=f(i);function i(e){return t(this,i),n.call(this,e)}return r(i,[{key:"search",value:function(e){var t=e===this.pattern;return{isMatch:t,score:t?0:1,indices:[0,this.pattern.length-1]}}}],[{key:"type",get:function(){return"exact"}},{key:"multiRegex",get:function(){return/^="(.*)"$/}},{key:"singleRegex",get:function(){return/^=(.*)$/}}]),i}(K),J=function(e){a(i,e);var n=f(i);function i(e){return t(this,i),n.call(this,e)}return r(i,[{key:"search",value:function(e){var t=-1===e.indexOf(this.pattern);return{isMatch:t,score:t?0:1,indices:[0,e.length-1]}}}],[{key:"type",get:function(){return"inverse-exact"}},{key:"multiRegex",get:function(){return/^!"(.*)"$/}},{key:"singleRegex",get:function(){return/^!(.*)$/}}]),i}(K),T=function(e){a(i,e);var n=f(i);function i(e){return t(this,i),n.call(this,e)}return r(i,[{key:"search",value:function(e){var t=e.startsWith(this.pattern);return{isMatch:t,score:t?0:1,indices:[0,this.pattern.length-1]}}}],[{key:"type",get:function(){return"prefix-exact"}},{key:"multiRegex",get:function(){return/^\^"(.*)"$/}},{key:"singleRegex",get:function(){return/^\^(.*)$/}}]),i}(K),U=function(e){a(i,e);var n=f(i);function i(e){return t(this,i),n.call(this,e)}return r(i,[{key:"search",value:function(e){var t=!e.startsWith(this.pattern);return{isMatch:t,score:t?0:1,indices:[0,e.length-1]}}}],[{key:"type",get:function(){return"inverse-prefix-exact"}},{key:"multiRegex",get:function(){return/^!\^"(.*)"$/}},{key:"singleRegex",get:function(){return/^!\^(.*)$/}}]),i}(K),V=function(e){a(i,e);var n=f(i);function i(e){return t(this,i),n.call(this,e)}return r(i,[{key:"search",value:function(e){var t=e.endsWith(this.pattern);return{isMatch:t,score:t?0:1,indices:[e.length-this.pattern.length,e.length-1]}}}],[{key:"type",get:function(){return"suffix-exact"}},{key:"multiRegex",get:function(){return/^"(.*)"\$$/}},{key:"singleRegex",get:function(){return/^(.*)\$$/}}]),i}(K),B=function(e){a(i,e);var n=f(i);function i(e){return t(this,i),n.call(this,e)}return r(i,[{key:"search",value:function(e){var t=!e.endsWith(this.pattern);return{isMatch:t,score:t?0:1,indices:[0,e.length-1]}}}],[{key:"type",get:function(){return"inverse-suffix-exact"}},{key:"multiRegex",get:function(){return/^!"(.*)"\$$/}},{key:"singleRegex",get:function(){return/^!(.*)\$$/}}]),i}(K),G=function(e){a(i,e);var n=f(i);function i(e){var r,o=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},c=o.location,a=void 0===c?A.location:c,s=o.threshold,u=void 0===s?A.threshold:s,h=o.distance,f=void 0===h?A.distance:h,l=o.includeMatches,d=void 0===l?A.includeMatches:l,v=o.findAllMatches,g=void 0===v?A.findAllMatches:v,y=o.minMatchCharLength,p=void 0===y?A.minMatchCharLength:y,m=o.isCaseSensitive,k=void 0===m?A.isCaseSensitive:m,M=o.ignoreLocation,x=void 0===M?A.ignoreLocation:M;return t(this,i),(r=n.call(this,e))._bitapSearch=new z(e,{location:a,threshold:u,distance:f,includeMatches:d,findAllMatches:g,minMatchCharLength:p,isCaseSensitive:k,ignoreLocation:x}),r}return r(i,[{key:"search",value:function(e){return this._bitapSearch.searchIn(e)}}],[{key:"type",get:function(){return"fuzzy"}},{key:"multiRegex",get:function(){return/^"(.*)"$/}},{key:"singleRegex",get:function(){return/^(.*)$/}}]),i}(K),H=function(e){a(i,e);var n=f(i);function i(e){return t(this,i),n.call(this,e)}return r(i,[{key:"search",value:function(e){for(var t,n=0,r=[],i=this.pattern.length;(t=e.indexOf(this.pattern,n))>-1;)n=t+i,r.push([t,n-1]);var o=!!r.length;return{isMatch:o,score:o?1:0,indices:r}}}],[{key:"type",get:function(){return"include"}},{key:"multiRegex",get:function(){return/^'"(.*)"$/}},{key:"singleRegex",get:function(){return/^'(.*)$/}}]),i}(K),Q=[W,H,T,U,B,V,J,G],X=Q.length,Y=/ +(?=([^\"]*\"[^\"]*\")*[^\"]*$)/;function Z(e){var t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};return e.split("|").map((function(e){for(var n=e.trim().split(Y).filter((function(e){return e&&!!e.trim()})),r=[],i=0,o=n.length;i<o;i+=1){for(var c=n[i],a=!1,s=-1;!a&&++s<X;){var u=Q[s],h=u.isMultiMatch(c);h&&(r.push(new u(h,t)),a=!0)}if(!a)for(s=-1;++s<X;){var f=Q[s],l=f.isSingleMatch(c);if(l){r.push(new f(l,t));break}}}return r}))}var ee=new Set([G.type,H.type]),te=function(){function e(n){var r=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},i=r.isCaseSensitive,o=void 0===i?A.isCaseSensitive:i,c=r.includeMatches,a=void 0===c?A.includeMatches:c,s=r.minMatchCharLength,u=void 0===s?A.minMatchCharLength:s,h=r.ignoreLocation,f=void 0===h?A.ignoreLocation:h,l=r.findAllMatches,d=void 0===l?A.findAllMatches:l,v=r.location,g=void 0===v?A.location:v,y=r.threshold,p=void 0===y?A.threshold:y,m=r.distance,k=void 0===m?A.distance:m;t(this,e),this.query=null,this.options={isCaseSensitive:o,includeMatches:a,minMatchCharLength:u,findAllMatches:d,ignoreLocation:f,location:g,threshold:p,distance:k},this.pattern=o?n:n.toLowerCase(),this.query=Z(this.pattern,this.options)}return r(e,[{key:"searchIn",value:function(e){var t=this.query;if(!t)return{isMatch:!1,score:1};var n=this.options,r=n.includeMatches;e=n.isCaseSensitive?e:e.toLowerCase();for(var i=0,o=[],c=0,a=0,s=t.length;a<s;a+=1){var u=t[a];o.length=0,i=0;for(var h=0,f=u.length;h<f;h+=1){var d=u[h],v=d.search(e),g=v.isMatch,y=v.indices,p=v.score;if(!g){c=0,i=0,o.length=0;break}if(i+=1,c+=p,r){var m=d.constructor.type;ee.has(m)?o=[].concat(l(o),l(y)):o.push(y)}}if(i){var k={isMatch:!0,score:c/i};return r&&(k.indices=o),k}}return{isMatch:!1,score:1}}}],[{key:"condition",value:function(e,t){return t.useExtendedSearch}}]),e}(),ne=[];function re(e,t){for(var n=0,r=ne.length;n<r;n+=1){var i=ne[n];if(i.condition(e,t))return new i(e,t)}return new z(e,t)}var ie="$and",oe="$or",ce="$path",ae="$val",se=function(e){return!(!e[ie]&&!e[oe])},ue=function(e){return!!e[ce]},he=function(e){return!v(e)&&m(e)&&!se(e)},fe=function(e){return i({},ie,Object.keys(e).map((function(t){return i({},t,e[t])})))},le=function(){function e(n){var r=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},i=arguments.length>2?arguments[2]:void 0;t(this,e),this.options=c({},A,{},r),this.options.useExtendedSearch,this._keyStore=new _(this.options.keys),this.setCollection(n,i)}return r(e,[{key:"setCollection",value:function(e,t){if(this._docs=e,t&&!(t instanceof E))throw new Error("Incorrect 'index' type");this._myIndex=t||$(this.options.keys,this._docs,{getFn:this.options.getFn})}},{key:"add",value:function(e){k(e)&&(this._docs.push(e),this._myIndex.add(e))}},{key:"remove",value:function(){for(var e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:function(){return!1},t=[],n=0,r=this._docs.length;n<r;n+=1){var i=this._docs[n];e(i,n)&&(this.removeAt(n),n-=1,r-=1,t.push(i))}return t}},{key:"removeAt",value:function(e){this._docs.splice(e,1),this._myIndex.removeAt(e)}},{key:"getIndex",value:function(){return this._myIndex}},{key:"search",value:function(e){var t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},n=t.limit,r=void 0===n?-1:n,i=this.options,o=i.includeMatches,c=i.includeScore,a=i.shouldSort,s=i.sortFn,u=i.ignoreFieldNorm,h=g(e)?g(this._docs[0])?this._searchStringList(e):this._searchObjectList(e):this._searchLogical(e);return de(h,{ignoreFieldNorm:u}),a&&h.sort(s),y(r)&&r>-1&&(h=h.slice(0,r)),ve(h,this._docs,{includeMatches:o,includeScore:c})}},{key:"_searchStringList",value:function(e){var t=re(e,this.options),n=this._myIndex.records,r=[];return n.forEach((function(e){var n=e.v,i=e.i,o=e.n;if(k(n)){var c=t.searchIn(n),a=c.isMatch,s=c.score,u=c.indices;a&&r.push({item:n,idx:i,matches:[{score:s,value:n,norm:o,indices:u}]})}})),r}},{key:"_searchLogical",value:function(e){var t=this,n=function(e,t){var n=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{},r=n.auto,i=void 0===r||r,o=function e(n){var r=Object.keys(n),o=ue(n);if(!o&&r.length>1&&!se(n))return e(fe(n));if(he(n)){var c=o?n[ce]:r[0],a=o?n[ae]:n[c];if(!g(a))throw new Error(b(c));var s={keyId:j(c),pattern:a};return i&&(s.searcher=re(a,t)),s}var u={children:[],operator:r[0]};return r.forEach((function(t){var r=n[t];v(r)&&r.forEach((function(t){u.children.push(e(t))}))})),u};return se(e)||(e=fe(e)),o(e)}(e,this.options),r=this._myIndex.records,i={},o=[];return r.forEach((function(e){var r=e.$,c=e.i;if(k(r)){var a=function e(n,r,i){if(!n.children){var o=n.keyId,c=n.searcher,a=t._findMatches({key:t._keyStore.get(o),value:t._myIndex.getValueForItemAtKeyId(r,o),searcher:c});return a&&a.length?[{idx:i,item:r,matches:a}]:[]}switch(n.operator){case ie:for(var s=[],u=0,h=n.children.length;u<h;u+=1){var f=e(n.children[u],r,i);if(!f.length)return[];s.push.apply(s,l(f))}return s;case oe:for(var d=[],v=0,g=n.children.length;v<g;v+=1){var y=e(n.children[v],r,i);if(y.length){d.push.apply(d,l(y));break}}return d}}(n,r,c);a.length&&(i[c]||(i[c]={idx:c,item:r,matches:[]},o.push(i[c])),a.forEach((function(e){var t,n=e.matches;(t=i[c].matches).push.apply(t,l(n))})))}})),o}},{key:"_searchObjectList",value:function(e){var t=this,n=re(e,this.options),r=this._myIndex,i=r.keys,o=r.records,c=[];return o.forEach((function(e){var r=e.$,o=e.i;if(k(r)){var a=[];i.forEach((function(e,i){a.push.apply(a,l(t._findMatches({key:e,value:r[i],searcher:n})))})),a.length&&c.push({idx:o,item:r,matches:a})}})),c}},{key:"_findMatches",value:function(e){var t=e.key,n=e.value,r=e.searcher;if(!k(n))return[];var i=[];if(v(n))n.forEach((function(e){var n=e.v,o=e.i,c=e.n;if(k(n)){var a=r.searchIn(n),s=a.isMatch,u=a.score,h=a.indices;s&&i.push({score:u,key:t,value:n,idx:o,norm:c,indices:h})}}));else{var o=n.v,c=n.n,a=r.searchIn(o),s=a.isMatch,u=a.score,h=a.indices;s&&i.push({score:u,key:t,value:o,norm:c,indices:h})}return i}}]),e}();function de(e,t){var n=t.ignoreFieldNorm,r=void 0===n?A.ignoreFieldNorm:n;e.forEach((function(e){var t=1;e.matches.forEach((function(e){var n=e.key,i=e.norm,o=e.score,c=n?n.weight:null;t*=Math.pow(0===o&&c?Number.EPSILON:o,(c||1)*(r?1:i))})),e.score=t}))}function ve(e,t){var n=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{},r=n.includeMatches,i=void 0===r?A.includeMatches:r,o=n.includeScore,c=void 0===o?A.includeScore:o,a=[];return i&&a.push(R),c&&a.push(F),e.map((function(e){var n=e.idx,r={item:t[n],refIndex:n};return a.length&&a.forEach((function(t){t(e,r)})),r}))}return le.version="6.4.3",le.createIndex=$,le.parseIndex=function(e){var t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},n=t.getFn,r=void 0===n?A.getFn:n,i=e.keys,o=e.records,c=new E({getFn:r});return c.setKeys(i),c.setIndexRecords(o),c},le.config=A,function(){ne.push.apply(ne,arguments)}(te),le},"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):(e=e||self).Fuse=t();;const idx=[{"link":"/about/","title":"About Me","body":"[About Me [My CV describing different things I‚Äôve done.] Add a note about me and then two cards: (1) cv in html, and (2) cv in pdf]".toLowerCase(),},{"link":"/links/","title":"Library","body":"[I got 99 Chrome tabs... Go to Links and References for places I like to reference for research &amp; help.]".toLowerCase(),},{"link":"/docs/","title":"Getting Started","body":"[Prerequisites and installation Install Hugo You need a recent extended version (we recommend version 0.61 or later) of Hugo to do local builds and previews of sites (like this one) that uses this theme. If you install from the release page, make sure to get the extended Hugo version, which supports sass; you may need to scroll down the list of releases to see it. For comprehensive Hugo documentation, see gohugo.io.]".toLowerCase(),},{"link":"/post/","title":"Posts","body":"[This is a series of exploratory analyses, articles, and utilities.]".toLowerCase(),},{"link":"/docs/overview/","title":"Overview","body":"[Instead of writing all your site pages from scratch, Hugo lets you define and use shortcodes. Why shortcodes? While markdown is sufficient to produce simple pages, it's insufficient where complex page structures are needed. Thusly, whenever we need special styling, shortcodes compliment the shortcomings of markdown. This way, you can side step complex html and css boilerplate in your content files.]".toLowerCase(),},{"link":"/app/","title":"Apps","body":"[my favorite shiny app with cool home ranging models and time lapse tiger tracking static flexdashboard page that i use for fast chart and map snippets -- shiny app i created to demo view covid data updates using r and js one of my first apps - trying to get my hands around interactivity]".toLowerCase(),},{"link":"/links/list/","title":"Links and References","body":"[Sources I like to use when looking for data or for researching/sharing geographic &amp; statistical concepts. I'm fully aware that googling is faster than scrolling through this ridiculous list, but it makes me feel better about closing Chrome tabs at the end of the day. Blogdown &amp; Hugo Hugo Shortcodes Scientific and Technical Blogging: Radix vs. Blogdown Alison Hill: Blog Work with RStudio, GitHub and Netlify to create and deploy your own webpage Cheatsheets Cheatsheets for Data Scientists (datacamp) Colors in R (pdf) R Markdown Statistics Cheat Sheet (web.mit.edu) joins using dplyr (r) R to python data wrangling snippets Beautiful Plotting in ggplot2 - ZevRoss (R) BI Visual Reference Visual Vocabulary - Financial Times Mapping Data to Visualizations Visualizing Percentages and Parts of a Whole How to Think Visually (Vital) Which Visualization? A Quick Reference (Franconeri) 10 Commandments of Visualization (DesignMantic) The Right Viz For the Right Data Statistical Symbols K-Means Clustering Markdown Code Chunks (r) knitr options Databases Visual Explanation of SQL joins (codinghorror) PostGIS - Reference PostgreSQL - Arrays PostgreSQL - Array Functions and Operators PostgreSQL - Geometric Types PostgreSQL - JSON Functions and Operators Well-known text representation of geometry (wiki) Data Model vs Data Dictionary vs Database Schema vs ERD Logical vs Physical Data Dictionary pyspark.sql module - Apache Spark (python) pyspark.rdd - Apache Spark (python) Querying Data with Apache Drill Faster Operations with the JSONB Data Type in PostgreSQL SQL vs NoSQL: 7 Key Takeaways Data Science Programming Cookbook for R Handling Strings with R (r) Custom interactive CSS/HTML tooltips with ggplot, shiny and R (r) Python Data Structures (python.org) Python stdtypes (python.org) Python Import System (python.org) Beautiful Soup Documentation scikit-learn NumPy - Routine Statistics What are Lambda Functions? (python) Interfacing R from a Python 3 Jupyter Notebook Statistical functions (scipy.stats) (python) stackoverflow Mathematical statistics functions (python) R Markdown Cookbook (yihui) Twitter Topic Modeling (python) Twitter Topics (r) Trump tweets: Topic Modeling using Latent Dirichlet Allocation News headlines text analysis (r) r/rprogramming r/rlanguage r/datascience R-bloggers A Data Scientific Method Foundational Methodology for Data Science KD nuggets Quick R by Datacamp Data sources Armed Conflict Location &amp; Event Data Project census.gov NOAA Datasets Waterpoint Data (WPDX) Humanitarian Data Exchange (OCHA) Relief Web Labs (OCHA) Taxonomy as a Service (OCHA) Data.gov - Ocean Data Saildrone Data Galapagos Vital Signs Resources UNICEF Data Awesome Public Datasets (GitHub) Twitter API AIRBUS TerraSAR-X Archive DRYAD UNESCO - Great Barrier Reef NASA Earth Data NASA Data Catalog NASA Data by Subject Development Awesome R Shiny Chrome DevTools shinyWidgets app - dreamrs (r) Shiny Authentication and User Management (r) Is there any way to hos R shiny apps with shinyapp.io with single sign-on authentication (SAML2.0 or openID connect)? OpenID Connect (OAuth2) - RStudio Connect: Admin Guide (r) OpenID Connect (OIDC) - shinyproxy.io (r) Welcome to OpenID Connect W3 Schools Netlify Transform a folder as git project synchronized on Github or Gitlab How to create an analytics dashboard in a Django app (python) Building a data-driven CV with R (r) Django CMS Mastering Shiny (wickham) Cloud Foundry - Deploying an app with Docker Docker Curriculem Building Docker Images with heroku.yml (Heroku Dev Center) How to Deploy a Web app Using Docker Web Server (DevTeam.Space) Twitter Developer Portal Paul Vanderlaken: Blog Geographic leaflet (r) SymbolixAU/mapdeck (r) globe4r (r) Gravity/Huff model tools PostGIS GeoDjango (python) leaflet extras (js) maptimize r/gis Learning Resources Wolfram Alpha Kaggle Khan Academy DataCamp 3Blue1Brown ZevRoss Tech Blog Linux and Cloud Install Ubuntu Desktop (Ubuntu Tutorials) Install JupyterHub on DigitalOcean Wget Command in Linux with Examples Machine Learning and AI Attention and Augmented Recurrent Neural Networks (Distill) alpha-analysis - Data Science, Machine Learning and Predictive Analytics eran raviv - Blog, Statistics and Econometrics Machine Learning Glossary (Google Developers) Dive into Deep Learning - Book Image classification with TensorFlow Lite Model Maker Tree Based Algorithms: A Complete Tutorial from Scratch (in R &amp; Python) A Tour of Machine Learning Algorithms (machinelearningmastery) Comparing machine learning classifiers based on their hyperplanes or decision boundaries (r) How To Choose The Right Test Options When Evaluating Machine Learning Algorithms (machinelearningmastery) r/learnmachinelearning r/machinelearning Natural Language Processing tidytextmining (r) 14 Best Natural Language Processing Tools in the World Today (linuxlinks) NLTK (python) spaCy (python) spacyr (r wrapper) tidytext (r) Stanford CoreNLP (java) pytorch-transformers MITIE MITIE trainer Research Global Climate Change (NASA) Forensic-Architecture.org OCHA Services - reliefweb Labs Johns Hopkins Coronavirus Resource Center Data and code for: Dietzel et al. Long-term shifts in the colony size structure of coral populations along the Great Barrier Reef #onemilliontweemap Clusterr r/science r/askscience Tweet Sentiment Visualization Statistics r/stats r/askstatisics r/statistics onlinestatbook (Rice University) Linear Regression (r-statistics.co) ggPredict() - Visualize multiple regression model (r) Significance of categorical predictor in logistic regression scikit-learn - choosing the right estimator Measures of Variability: Coefficient of Variation, Variance, and Standard Deviation Probability and Statistics Concepts (CK12) Hierarchical Networks (r) Kernel Density Estimation (wiki) Rigorous home range estimation with movement data: a new autocorrelated kernel density estimator Combinations and Permutations 10 Things to Know about Confidence Intervals Bootstrap Confidence Intervals (R) Calculating Line Regression by Hand Central Limit Theorem (Stat Trek) Confidence Intervals Covariance Analysis (ANCOVA) Demystifying Bayesian Deep Learning (Eric J. Ma) Generalized Linear Models in R, Part 2: Understanding Model Fit in Logistic Regression Output ] How Bayesian Inference Works How A/B Testing Works (for Non-Mathematicians) How To Estimate Model Accuracy in R Using The Caret Package (machinelearningmastery) How to interpret a ROC curve? Index for Calculus (math words) In linear regression, when is it appropriate to use the log of an independent variable instead of the actual values? Median Absolute Deviation Pareto Efficiency P-Value ‚ÄúFormula‚Äù, Testing Your Hypothesis Random variables and probability distributions ROC curves and Area Under the Curve explained (video) Simple guide to confusion matrix terminology Statistical Soup: ANOVA, ANCOVA, MANOVA, &amp; MANCOVA Summation Notation T Test Calculator T Test (Student‚Äôs T-Test): Definition and Examples Two Sample t Test: equal variances What is the meaning of p values and t values in statistical tests? What are kernels in machine learning and SVM and why do we need them? What is a Tensor What Are T Values and P Values in Statistics? Understanding Hypothesis Tests: Significance Levels (Alpha) and P values in Statistics Understanding t-Tests: 1-sample, 2-sample, and Paired t-Tests Tips, Tools, References 23 RStudio Tips, Tricks, and Shortcuts (Dataquest) emo(ji) favicon.io favicon converter Markdown Guide Pandoc User's Guide R Markdown Basics flowcharts in R RMarkdown Driven Development (RmdDD) How to simplify your code by using data flows (R) freelogodesign.org How to add gifs to markdown documents (SO) Content Management in Hugo (gohugo.io) stackedit http statuses colorbrewer pimp my r markdown SQL Fiddle Pablo Trading The R Trader - Machine learning with Python and R for quantitative finance FOSS Trading R for Metatrader Build A Cryptocurrency Trading Bot with R How to Build an Automated Trading System using R Building a machine learning based Crypto trading bot in R Quantocracy tr8dr Visualization D3 Gallery Obervable HQ (js) ggplot for python ggplot2 tidyverse (r) r2d3 - R Interface to D3 Visualizations (r) teacherflows sankey shiny vis (r) r graph gallery (Holtz) D3 graph gallery (Holtz) python graph gallery (Holtz) From data to vis - chart selection (Holtz) From data to vis - caveats (Holtx) How to draw connecting routes on map with R Google Charts Gallery data.tree vignette (r) Interactive Network Visualization (r) 1000 D3 JS Examples - Techslides (js) D3 Force Network Testing Group - bl.ocks.org (js) Interactive Data VIsuaizations with Shiny - bioconductor.org (r) Data visualization: basic principles billboard.js (js) r/dataisbeautiful]".toLowerCase(),},{"link":"/docs/shortcodes/","title":"Shortcodes","body":"[Why shortcodes? While markdown is sufficient to produce simple pages, it's insufficient where complex page structures are needed. Thusly, whenever we need special styling, shortcodes compliment the shortcomings of markdown. This way, you can side step complex html and css boilerplate in your content files. Sometimes, the shortcode will wrap content, sometimes it won't. When content is wrapped, a closing shortcode tag is needed. Please see the link I provided above and the markdown files for examples. You'll get the gist pretty quickly. I've setup the following shortcodes: Block Takes positional modifiers Example 1 2 3 4 5 ... {{&lt; block &#34;modifiers&#34; &gt;}} &lt;!-- Nest columns or content --&gt; {{&lt; /block &gt;}} ... Column It takes positional parameters Example 1 2 3 {{&lt; column &#34;mt-2 mb-2&#34; &gt;}} &lt;!-- applied margin top and margin bottom modifiers --&gt; {{&lt; /column &gt;}} Video This allows you to embed a youtube video in you content. You would achieve that using a positional parameter (needs no name )parameter, like so: Example: 1 2 {{&lt; youtube &#34;xWF59rWSceA&#34; &gt;}} &lt;!-- Use the youtube video id --&gt; Button This adds a styled link (styled like a button). It takes two no-optional parameters: PARAMETER PURPOSE OPTIONAL label button text no url button link no Example {{&lt; button url=&quot;/&quot; label=&quot;doe nu mee&quot; &gt;}} Picture You want to use darkmode images when darkmode is enabled on a device and a regular image on lightmode? It takes 3 positional parameter Store these images in the static/images directory. ... {{&lt; picture &quot;lightModeImage.png&quot; &quot;darkModeImage.png&quot; &quot;Image alt text&quot; &gt;}} ...]".toLowerCase(),},{"link":"/docs/shortcodes-applied/","title":"Shortcodes Applied","body":"[Using blocks, columns &amp; buttons 1 2 3 4 5 6 7 8 9 10 11 12 13 {{&lt; block &#34;grid-2&#34; &gt;}} {{&lt; column &gt;}} #### Coumn 1 Lorem ipsum dolor sit amet, ... {{&lt; button &#34;https://github.com/onweru/compose&#34; &#34;Download Theme&#34; &gt;}} {{&lt; /column &gt;}} {{&lt; column &gt;}} &lt;!-- generates üëá --&gt; Coumn 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Download Theme Coumn 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Read the Docs Youtube 1 2 {{&lt; youtube &#34;q0hyYWKXF0Q&#34; &gt;}} &lt;!-- generates üëá --&gt; Picture 1 2 {{&lt; picture &#34;compose.svg&#34; &#34;compose-light.svg&#34; &#34;Compose Logo&#34; &gt;}} &lt;!-- generates üëá --&gt;]".toLowerCase(),},{"link":"/2020/12/30/custom-color-palette/","title":"Custom Color Palette in R","body":"[Updated 2021-01-02. I'm describing customizing my own color palette for ggplot2. This is based off of Creating corporate colour palettes for ggplot2 by drsimonj. First, find a bunch of colors that you like. Canva is a good place to go for inspiration; or, us this tool from Adobe Color that will extract colors from a photo you like. Then, there's always colorbrewer2. Note: In some cases, it's enough to use the scale_color_manual() and scale_fill_manual() functions directly within ggplot2. See the docs for examples. Define colors This is the color scheme used by the non-profit, splashdown.org. 1 2 3 4 5 6 7 8 9 10 splashdown_colors &lt;- c( `ebony` = &#34;#1E2225&#34;, `blue_grotto` = &#34;#05263B&#34;, `charcoal` = &#34;#43758A&#34;, `light_sea_green` = &#34;#36B4AF&#34;, `pewter` = &#34;#D3DADA&#34;, `carafe` = &#34;#55423A&#34;, `goldenrod` = &#34;#F1B416&#34;, `burnt_sienna` = &#34;#E56A19&#34;, `crimson` = &#34;#900000&#34;) Function to extract colors as hex codes (source: drsimonj) 1 2 3 4 5 6 splashdown_cols &lt;- function(...) { cols &lt;- c(...) if (is.null(cols)) return (splashdown_colors) splashdown_colors[cols] } Examples of returning specific color codes. The scales package has a convenient show_col() function for this: 1 2 3 4 5 6 library(scales) library(dplyr) par(mfrow=c(1,2)) splashdown_cols() %&gt;% show_col() splashdown_cols(&#34;light_sea_green&#34;, &#34;burnt_sienna&#34;) %&gt;% show_col() Very simple plot with color selection. 1 2 3 ggplot(mtcars, aes(hp, mpg)) + geom_point(color = splashdown_cols(&#34;goldenrod&#34;), size = 4, alpha = .8) Create palettes Combine sets of colors to create individual palettes. 1 2 3 4 5 splashdown_palettes &lt;- list( `primary` = splashdown_cols(&#34;ebony&#34;, &#34;blue_grotto&#34;, &#34;charcoal&#34;, &#34;light_sea_green&#34;, &#34;pewter&#34;), `secondary` = splashdown_cols(&#34;carafe&#34;, &#34;goldenrod&#34;, &#34;burnt_sienna&#34;, &#34;crimson&#34;), `all` = splashdown_cols(&#34;ebony&#34;, &#34;blue_grotto&#34;, &#34;charcoal&#34;, &#34;light_sea_green&#34;, &#34;pewter&#34;, &#34;carafe&#34;, &#34;goldenrod&#34;, &#34;burnt_sienna&#34;, &#34;crimson&#34;) ) Function to interpolate the palettes (source: drsimonj). Option to add additional arguments into colorRampPallete() like alpha. 1 2 3 4 5 splashdown_pal &lt;- function(palette = &#34;main&#34;, reverse = FALSE, ...) { pal &lt;- splashdown_palettes[[palette]] if (reverse) pal &lt;- rev(pal) colorRampPalette(pal, ...) } Preview the palettes, e.g. applying 10 levels to the primary color palette. Again using scales::show_col() to display the colors with hex codes. 1 2 3 4 par(mfrow=c(1,2)) splashdown_pal(&#34;primary&#34;)(12) %&gt;% show_col() splashdown_pal(&#34;secondary&#34;)(4) %&gt;% show_col() Scale functions for ggplot2 Everything that's been up to this point is creating a storage places for favorite colors. Now, prepare some functions specifically for using with ggplot. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 scale_color_splashdown &lt;- function(palette = &#34;main&#34;, discrete = TRUE, reverse = FALSE, ...) { pal &lt;- splashdown_pal(palette = palette, reverse = reverse) if (discrete) { discrete_scale(&#34;colour&#34;, paste0(&#34;splashdown_&#34;, palette), palette = pal, ...) } else { scale_color_gradientn(colours = pal(256), ...) } } scale_fill_splashdown &lt;- function(palette = &#34;main&#34;, discrete = TRUE, reverse = FALSE, ...) { pal &lt;- splashdown_pal(palette = palette, reverse = reverse) if (discrete) { discrete_scale(&#34;fill&#34;, paste0(&#34;splashdown_&#34;, palette), palette = pal, ...) } else { scale_fill_gradientn(colours = pal(256), ...) } } Example plots 1 2 3 ggplot(iris, aes(Sepal.Width, Sepal.Length, color = Sepal.Length)) + geom_point(size = 4, alpha = .6) + scale_color_splashdown(discrete = FALSE, palette = &#34;primary&#34;) 1 2 3 ggplot(iris, aes(Sepal.Width, Sepal.Length, color = Species)) + geom_point(size = 4) + scale_color_splashdown(&#34;secondary&#34;) 1 2 3 4 ggplot(mpg, aes(manufacturer, fill = manufacturer)) + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + scale_fill_splashdown(palette = &#34;all&#34;, guide = &#34;none&#34;) Finally, I want to be able to pull down my palette whereever I am, so I copied the above R code into splashdown_col_pal.r and pushed it to my GitHub repo. I can read it down using devtools as an r file or as a gist. (devtools cheatsheet) 1 2 library(devtools) source_url(&#34;https://raw.githubusercontent.com/rjfranssen/rjfranssen/main/utils/splashdown_col_pal.r&#34;) Acknowledgements This post was made possible thanks to lubridate (Grolemund and Wickham, 2011) dplyr (Wickham, Fran√ßois, Henry, and M√ºller, 2020) ggplot2 (Wickham, 2016) BiocStyle (Oles, Morgan, and Huber, 2020) blogdown (Xie, Hill, and Thomas, 2017) devtools (Wickham, Hester, and Chang, 2020) knitcitations (Boettiger, 2020)]".toLowerCase(),},{"link":"/2020/12/30/twitter-sentiment-philly/","title":"Twitter Sentiment Philly","body":"[Updated 2021-01-02. This is a walkthrough of using the Twitter API with the rtweet R package. It borrows from a few difference sources, including: ropensci/rtweet documentation here Tidy Text Mining with R Twitter Data in R Using Rtweet: Analyze and Download Twitter Data Comparing Twitter archives Mining NASA metadata Analyzing usenet text Load libraries 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 library(rtweet) library(rmarkdown) library(NLP) library(tm) library(RColorBrewer) library(wordcloud) library(topicmodels) library(SnowballC) library(config) library(dplyr) library(tidytext) library(tidyr) library(ggplot2) library(lubridate) library(readr) library(skimr) Configure Twitter API access There are a couple different ways to authenticate your session. The rtweet package makes this pretty easy by enabling browser-based authentication. I prefer to authenticate in code. Check the rtweet vignette for connection options. What I did: Head over to the Twitter Developer site and requested a developer account to gain access to Twitter APIs and tools. Then I used a config.yml to store my keys. The config package a good way to import keys and passwords without making them publc (just make sure they're covered by your .gitignore file!). Check out the documentation. Note: If using Hugo and don't want it swept up into your public directory, you can also add config.yml to the ignoreFiles parameter in the config.toml. 1 2 3 4 5 6 7 8 9 config &lt;- config::get(file = &#39;config.yml&#39;) ## authenticate via web browser token &lt;- create_token( app = config$app, consumer_key = config$apikey, consumer_secret = config$apikeysecret, access_token = config$accesstoken, access_secret = config$accesstokensecret) Query the Tweets! Going to ask for the most recent 2000. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # Search tweets using rtweet search function tweets_df &lt;- rtweet::search_tweets( q = &#39;philadelphia philly&#39; , n = 2000 , include_rts = FALSE , type = &#39;recent&#39; #, geocode = &#34;37.78, -122.40, 1mi&#34; #, retryonratelimit = TRUE #, parse = FALSE #, lang = &#39;en&#39; # ... see Twitter&#39;s REST API for more search parameters ) ## Create lat/lng variables using all available tweet and profile geo-location data tweets_df &lt;- rtweet::lat_lng(tweets_df) ## Also going to create some friendlier date variables;`created_at` is a `POSIXct` type that requires a bit of tender loving care. WIll still keep the `created_at` value, which works great with `dygraphs` and just fine with `ggplot2`, but I want to be able to group by date variables. tweets_df$tweet_datetime &lt;- tweets_df$created_at tweets_df$tweet_date &lt;- tweets_df$created_at %&gt;% as.POSIXlt() %&gt;% as.Date() %&gt;% ymd() tweets_df$tweet_year &lt;- tweets_df$created_at %&gt;% as.POSIXlt() %&gt;% as.Date() %&gt;% year() tweets_df$tweet_month &lt;- tweets_df$created_at %&gt;% as.POSIXlt() %&gt;% as.Date() %&gt;% month() tweets_df$tweet_day &lt;- tweets_df$created_at %&gt;% as.POSIXlt() %&gt;% as.Date() %&gt;% day() Optionally, save this df to a local directory so you come back to this later without having to hit the Twitter API; Rds and Rdata files load super fast. You can also add these file types to your .gitignore and config.toml if you don't want to push them to your online git repo. 1 saveRDS(tweets_df, file = &#34;tweets_df.Rds&#34;, compress = &#39;xz&#39;) Reading back in the .Rds file.. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 tweets_df &lt;- readRDS(&#34;tweets_df.Rds&#34;) tweets_df %&gt;% head() ## # A tibble: 6 x 97 ## user_id status_id created_at screen_name text source ## &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 142219~ 13439700~ 2020-12-29 17:20:08 PhillyInqu~ &#34;Uni~ Socia~ ## 2 142219~ 13421567~ 2020-12-24 17:14:43 PhillyInqu~ &#34;It&#39;~ Socia~ ## 3 142219~ 13432591~ 2020-12-27 18:15:04 PhillyInqu~ &#34;Tod~ Socia~ ## 4 142219~ 13418156~ 2020-12-23 18:39:19 PhillyInqu~ &#34;COV~ Socia~ ## 5 142219~ 13421897~ 2020-12-24 19:25:49 PhillyInqu~ &#34;New~ Socia~ ## 6 142219~ 13418101~ 2020-12-23 18:17:18 PhillyInqu~ &#34;&lt;U+274C&gt; S~ Socia~ ## # ... with 91 more variables: display_text_width &lt;dbl&gt;, ## # reply_to_status_id &lt;chr&gt;, reply_to_user_id &lt;chr&gt;, ## # reply_to_screen_name &lt;chr&gt;, is_quote &lt;lgl&gt;, is_retweet &lt;lgl&gt;, ## # favorite_count &lt;int&gt;, retweet_count &lt;int&gt;, quote_count &lt;int&gt;, ## # reply_count &lt;int&gt;, hashtags &lt;list&gt;, symbols &lt;list&gt;, urls_url &lt;list&gt;, ## # urls_t.co &lt;list&gt;, urls_expanded_url &lt;list&gt;, media_url &lt;list&gt;, ## # media_t.co &lt;list&gt;, media_expanded_url &lt;list&gt;, media_type &lt;list&gt;, ## # ext_media_url &lt;list&gt;, ext_media_t.co &lt;list&gt;, ext_media_expanded_url &lt;list&gt;, ## # ext_media_type &lt;chr&gt;, mentions_user_id &lt;list&gt;, mentions_screen_name &lt;list&gt;, ## # lang &lt;chr&gt;, quoted_status_id &lt;chr&gt;, quoted_text &lt;chr&gt;, ## # quoted_created_at &lt;dttm&gt;, quoted_source &lt;chr&gt;, quoted_favorite_count &lt;int&gt;, ## # quoted_retweet_count &lt;int&gt;, quoted_user_id &lt;chr&gt;, quoted_screen_name &lt;chr&gt;, ## # quoted_name &lt;chr&gt;, quoted_followers_count &lt;int&gt;, ## # quoted_friends_count &lt;int&gt;, quoted_statuses_count &lt;int&gt;, ## # quoted_location &lt;chr&gt;, quoted_description &lt;chr&gt;, quoted_verified &lt;lgl&gt;, ## # retweet_status_id &lt;chr&gt;, retweet_text &lt;chr&gt;, retweet_created_at &lt;dttm&gt;, ## # retweet_source &lt;chr&gt;, retweet_favorite_count &lt;int&gt;, ## # retweet_retweet_count &lt;int&gt;, retweet_user_id &lt;chr&gt;, ## # retweet_screen_name &lt;chr&gt;, retweet_name &lt;chr&gt;, ## # retweet_followers_count &lt;int&gt;, retweet_friends_count &lt;int&gt;, ## # retweet_statuses_count &lt;int&gt;, retweet_location &lt;chr&gt;, ## # retweet_description &lt;chr&gt;, retweet_verified &lt;lgl&gt;, place_url &lt;chr&gt;, ## # place_name &lt;chr&gt;, place_full_name &lt;chr&gt;, place_type &lt;chr&gt;, country &lt;chr&gt;, ## # country_code &lt;chr&gt;, geo_coords &lt;list&gt;, coords_coords &lt;list&gt;, ## # bbox_coords &lt;list&gt;, status_url &lt;chr&gt;, name &lt;chr&gt;, location &lt;chr&gt;, ## # description &lt;chr&gt;, url &lt;chr&gt;, protected &lt;lgl&gt;, followers_count &lt;int&gt;, ## # friends_count &lt;int&gt;, listed_count &lt;int&gt;, statuses_count &lt;int&gt;, ## # favourites_count &lt;int&gt;, account_created_at &lt;dttm&gt;, verified &lt;lgl&gt;, ## # profile_url &lt;chr&gt;, profile_expanded_url &lt;chr&gt;, account_lang &lt;lgl&gt;, ## # profile_banner_url &lt;chr&gt;, profile_background_url &lt;chr&gt;, ## # profile_image_url &lt;chr&gt;, lat &lt;dbl&gt;, lng &lt;dbl&gt;, tweet_datetime &lt;dttm&gt;, ## # tweet_date &lt;date&gt;, tweet_year &lt;dbl&gt;, tweet_month &lt;dbl&gt;, tweet_day &lt;int&gt; Examining data Now going to identify a few basic things, but first, this df has 92 columns. What are some of the columns that we're most likely to use? The skimr package is really good at doing this and I often use it in addition to summary(). 1 2 #summary(tweet_df) skim(tweets_df) And what does the table look like? 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 tweets_df %&gt;% select( # About the tweet text , lang , hashtags , symbols , favorite_count , retweet_count , lat , lng # About the user , name , screen_name , followers_count , friends_count , location , description , listed_count , statuses_count , created_at , account_created_at ) %&gt;% head(2) ## # A tibble: 2 x 18 ## text lang hashtags symbols favorite_count retweet_count lat lng name ## &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Unio~ de &lt;chr [1~ &lt;chr [~ 0 0 NA NA The ~ ## 2 It&#39;s~ en &lt;chr [1~ &lt;chr [~ 5 1 NA NA The ~ ## # ... with 9 more variables: screen_name &lt;chr&gt;, followers_count &lt;int&gt;, ## # friends_count &lt;int&gt;, location &lt;chr&gt;, description &lt;chr&gt;, listed_count &lt;int&gt;, ## # statuses_count &lt;int&gt;, created_at &lt;dttm&gt;, account_created_at &lt;dttm&gt; Sample stats &amp; visualizations What is our total tweet count? 1 2 nrow(tweets_df) %&gt;% print() ## [1] 1897 What is the most favorited tweet? 1 2 3 4 5 6 tweets_df %&gt;% slice_max(favorite_count, n = 1) %&gt;% # can also use arrange() select(screen_name , text , favorite_count , created_at) %&gt;% knitr::kable() screen_name text favorite_count created_at GetUpESPN &quot;Carson Wentz is soft! ... He don't have that Philly toughness ... But Jalen Hurts does! Jalen Hurts IS Philadelphia. Jalen Hurts is freakin' Rocky!&quot; ‚Äî@Realrclark25 https://t.co/nYvJ7IsB57 | 1993|2020-12-21 16:16:24 | What users are tweeting the most? 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 top_tweeters &lt;- tweets_df %&gt;% group_by(screen_name) %&gt;% summarize(num_tweets = n()) %&gt;% arrange(-num_tweets) %&gt;% # can also use slice_max() ungroup() %&gt;% head(5) ggplot(top_tweeters) + geom_histogram(aes(x = screen_name, y = num_tweets, fill = screen_name), stat = &#39;identity&#39;) + theme_minimal() + scale_fill_viridis_d(option = &#34;viridis&#34;) + labs(title = &#34;What users are tweeting the most?&#34;) + theme(legend.position = &#34;none&#34;) + xlab(&#34;&#34;) + ylab(&#34;Number of Tweets&#34;) In what languages are they tweeting? Languages are recorded here as alpha-3 / ISO 639.2 codes. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 tweets_df %&gt;% group_by(lang) %&gt;% summarize(num_tweets = n()) %&gt;% arrange(-num_tweets) %&gt;% # can also use slice_max() head(5) ## # A tibble: 5 x 2 ## lang num_tweets ## &lt;chr&gt; &lt;int&gt; ## 1 en 1829 ## 2 und 34 ## 3 cy 8 ## 4 es 6 ## 5 de 3 # ggplot(tweets_df) + # geom_bar(aes(x = lang, fill = lang)) What platforms are people using? 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 users_by_source &lt;- tweets_df %&gt;% group_by(source) %&gt;% summarize(num_users = n_distinct(user_id)) %&gt;% arrange(-num_users) %&gt;% # can also use slice_max() ungroup() %&gt;% head(10) ggplot(users_by_source) + geom_histogram(aes(x = reorder(source, num_users), y = num_users, fill = source), stat = &#39;identity&#39;) + scale_fill_viridis_d(option = &#34;viridis&#34;) + coord_flip() + theme_gray() + theme(legend.position = &#34;none&#34;) + #theme(legend.title = element_blank()) + labs(title = &#34;Top 10 Twitter Platforms People are Using&#34;) + xlab(&#34;&#34;) + ylab(&#34;Number of Users&#34;) + scale_y_continuous(breaks = seq(0, 400, by=50), limits = c(0, 400), expand = c(0, 0)) + # use expand() to remove empty space between axis and bars geom_hline(yintercept = seq(0, 400, by = 25), color = &#39;white&#39;) How many tweets per day? Going to create a chart with ggplot2 and plotly and a second one with dygraphs for r. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 tweets_per_day &lt;- tweets_df %&gt;% group_by(tweet_date) %&gt;% summarize(num_tweets = n()) %&gt;% arrange(-num_tweets) %&gt;% # can also use slice_max() ungroup() # Using ggplot + plotly library(plotly) plt1 &lt;- ggplot(tweets_per_day) + geom_line(aes(x = tweet_date, y = num_tweets)) + scale_fill_viridis_d(option = &#34;viridis&#34;) + theme_gray() + theme(legend.position = &#34;none&#34;) + labs(title = &#34;Daily Tweets for &#39;Philadelphia&#39; (ggplot2 + plotly)&#34;) + xlab(&#34;&#34;) + ylab(&#34;Number of Tweets&#34;) pltly1 &lt;- plotly::ggplotly(plt1) Note: for html widgets, because I'm writing this in .Rmarkdown, I use the below snippet to save the html widget to file and then bring it back in via an iframe. The main reason is that .Rmarkdown files get rendered as .markdown and then html, and my widgets were getting lost. Widgets work great with .Rmd files, but I lose some of the other hugo functionalities like the TOCs and code highlighting. This might change soon if it hasn't already. I've hidden these snippets for the other html widgets below. 1 2 3 4 5 6 7 8 widget &lt;- pltly1 widgetfile &lt;- &#39;pltly1.html&#39; htmlwidgets::saveWidget(widget = widget , selfcontained = TRUE , file = widgetfile) cat(paste0(&#39;&lt;iframe src= &#34;&#39;, widgetfile, &#39;&#34; width=&#34;100%&#34; height=&#34;400&#34; style=&#34;border: none;&#34;&gt;&lt;/iframe&gt;&#39;)) 1 2 3 4 5 6 7 8 # Using dygraph # convert series to an xts object tweets_ts &lt;- xts::xts(tweets_per_day$num_tweets, order.by = tweets_per_day$tweet_date) # Create the dygraph library(dygraphs) dygraph1 &lt;- dygraph(tweets_ts, main = &#34;Daily Tweets for &#39;Philadelphia&#39; (dygraph)&#34;) %&gt;% dySeries(&#34;V1&#34;, label = &#34;Tweet Count&#34;) And where are people tweeting? Of the 1897 tweets that we have, 133 are geotagged. We'll plot those using leaflet and build on this later. 1 2 3 4 5 6 library(leaflet) tweet_map &lt;- leaflet(tweets_df) %&gt;% addTiles() %&gt;% addCircleMarkers(lng = ~lng , lat = ~lat) Sentiment analysis Now switching over to tidytext The function get_sentiments() allows us to get specific sentiment lexicons with the appropriate measures for each one. Preview some of the sentiment lexicons. You'll be prompted to download the textdata package and the afinn, bing, and nrc lexicons if you don't have them already. afinn - assigns a numeric value to each word on a scale of -5 to +5 bing - binary assignment to each word, e.g. 'positive' or 'negative' nrc - categorizes words into categories: positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 #install.packages(&#34;textdata&#34;) library(tidytext) get_sentiments(&#34;afinn&#34;) %&gt;% head() ## # A tibble: 6 x 2 ## word value ## &lt;chr&gt; &lt;dbl&gt; ## 1 abandon -2 ## 2 abandoned -2 ## 3 abandons -2 ## 4 abducted -2 ## 5 abduction -2 ## 6 abductions -2 get_sentiments(&#34;bing&#34;) %&gt;% head() ## # A tibble: 6 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 2-faces negative ## 2 abnormal negative ## 3 abolish negative ## 4 abominable negative ## 5 abominably negative ## 6 abominate negative get_sentiments(&#34;nrc&#34;) %&gt;% head() ## # A tibble: 6 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 abacus trust ## 2 abandon fear ## 3 abandon negative ## 4 abandon sadness ## 5 abandoned anger ## 6 abandoned fear Now tidy up the data and prepare it for analysis. If running this with the austen_books() example from the tidytext docs, it's relatively straightforward: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 library(stringr) library(janeaustenr) tidy_books &lt;- austen_books() %&gt;% group_by(book) %&gt;% mutate( linenumber = row_number(), chapter = cumsum(str_detect(text, regex(&#34;^chapter [\\divxlc]&#34;, ignore_case = TRUE)))) %&gt;% ungroup() %&gt;% unnest_tokens(word, text) tidy_books %&gt;% head() %&gt;% knitr::kable() book linenumber chapter word Sense &amp; Sensibility 1 0 sense Sense &amp; Sensibility 1 0 and Sense &amp; Sensibility 1 0 sensibility Sense &amp; Sensibility 3 0 by Sense &amp; Sensibility 3 0 jane Sense &amp; Sensibility 3 0 austen But I want to run this on tweets, so I need to reshape the data. I only executed one search for philadelphia+philly, so I'm going to build a dataframe from that. Though, we can run multiple searches and assign each search its own book value to enable comparisons and faceted plots later. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 tweets_book &lt;- cbind(&#39;Philadelphia&#39;, tweets_df$text) %&gt;% data.frame() colnames(tweets_book) &lt;- c(&#39;book&#39;, &#39;text&#39;) # Reformat the date - running above insists on converting the `tweet_date` value to its integer form. tweets_book$chapter &lt;- tweets_df$tweet_date tidy_books &lt;- tweets_book %&gt;% group_by(book) %&gt;% mutate( linenumber = row_number() ) %&gt;% ungroup() %&gt;% unnest_tokens(word, text) tidy_books %&gt;% head() %&gt;% knitr::kable() book chapter linenumber word Philadelphia 2020-12-29 1 union Philadelphia 2020-12-29 1 to Philadelphia 2020-12-29 1 sell Philadelphia 2020-12-29 1 mark Philadelphia 2020-12-29 1 mckenzie Philadelphia 2020-12-29 1 to Now the tweets are in a tidy format with one word per row. Looking at the nrc lexicon, what are some of the &quot;joy&quot; words? 1 2 3 4 nrc_joy &lt;- get_sentiments(&#34;nrc&#34;) %&gt;% filter(sentiment == &#34;joy&#34;) head(nrc_joy) %&gt;% knitr::kable() word sentiment absolution joy abundance joy abundant joy accolade joy accompaniment joy accomplish joy How many &quot;joy&quot; words do we have in our tweets? 1 2 3 4 5 tidy_books %&gt;% filter(book == &#34;Philadelphia&#34;) %&gt;% inner_join(nrc_joy) %&gt;% count(word, sort = TRUE) %&gt;% head(10) %&gt;% knitr::kable() word n love 58 good 56 holiday 52 art 36 food 29 music 27 deal 24 save 24 safe 23 happy 21 Using tidyr, we can continue to examine how sentiment changes throughout the book. This is more relevant to the novels examples from the docs, but I'm including it here as an exercise. This example will use the bing lexicon. We need some type of interval - the example creates an index from 80 line chunks, but I'm going to update this to use the chapter value, which now represents the date of each tweet allowing to see changes in sentiment over time. From the docs: ... First, we find a sentiment score for each word using the Bing lexicon and inner_join(). Next, we count up how many positive and negative words there are in defined sections of each book. We define an index here to keep track of where we are in the narrative; this index (using integer division) counts up sections of 80 lines of text. The %/% operator does integer division (x %/% y is equivalent to floor(x/y)) so the index keeps track of which 80-line section of text we are counting up negative and positive sentiment in. Small sections of text may not have enough words in them to get a good estimate of sentiment while really large sections can wash out narrative structure. For these books, using 80 lines works well, but this can vary depending on individual texts, how long the lines were to start with, etc. We then use spread() so that we have negative and positive sentiment in separate columns, and lastly calculate a net sentiment (positive - negative). 1 2 3 4 5 6 philly_sentiment &lt;- tidy_books %&gt;% inner_join(get_sentiments(&#34;bing&#34;)) %&gt;% #count(book, index = linenumber %/% 80, sentiment) %&gt;% # 80-line tweet chunks count(book, index = chapter, sentiment) %&gt;% spread(sentiment, n, fill = 0) %&gt;% mutate(sentiment = positive - negative) 1 2 3 4 5 6 7 8 9 ggplot(philly_sentiment) + geom_col(aes(index, sentiment, fill = sentiment)) + #facet_wrap(~book, ncol = 1, scales = &#34;free_x&#34;) # apply if we&#39;relooking across multiple books + labs(title = &#34;Sentiment of &#39;Philadelphia&#39; Tweets&#34;) + scale_fill_gradient2(low=&#39;red&#39;, mid=&#39;orange&#39;, high=&#39;navy&#39;) + #scale_fill_viridis_c(option = &#34;viridis&#34;) + theme(legend.position = &#34;none&#34;) + xlab(&#34;&#34;) + ylab(&#34;Sentiment&#34;) Comparing sentiments Going to compare the different sentiment lexicons. Still writing the code as if there are multiple books (or search results) in the dataframe. 1 2 philly_tweets &lt;- tidy_books %&gt;% filter(book == &#34;Philadelphia&#34;) Using dplyr::inner_join(), estimate the different sentiment values for each lexicon. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## afinn afinn &lt;- philly_tweets %&gt;% inner_join(get_sentiments(&#34;afinn&#34;)) %&gt;% #group_by(index = linenumber %/% 80) %&gt;% # Can use 80-line chunks group_by(index = chapter) %&gt;% # or other type of index value, like date summarise(sentiment = sum(value)) %&gt;% mutate(method = &#34;AFINN&#34;) ## rbinding bing and nrc bing_and_nrc &lt;- bind_rows( philly_tweets %&gt;% inner_join(get_sentiments(&#34;bing&#34;)) %&gt;% mutate(method = &#34;Bing et al.&#34;), philly_tweets %&gt;% inner_join(get_sentiments(&#34;nrc&#34;) %&gt;% filter(sentiment %in% c(&#34;positive&#34;, &#34;negative&#34;)) ) %&gt;% mutate(method = &#34;NRC&#34;)) %&gt;% #count(method, index = linenumber %/% 80, sentiment) %&gt;% # Can use 80-line chunks count(method, index = chapter, sentiment) %&gt;% # or other type of index value, like date spread(sentiment, n, fill = 0) %&gt;% mutate(sentiment = positive - negative) Now combine and plot them together. 1 2 3 4 5 6 7 8 9 10 bind_rows(afinn, bing_and_nrc) %&gt;% ggplot(aes(index, sentiment, fill = method)) + geom_col(show.legend = FALSE) + facet_wrap(~method, ncol = 1, scales = &#34;free_y&#34;) + labs(title = &#34;Sentiment of &#39;Philadelphia&#39; Tweets - Lexicon Comparison&#34;) + scale_fill_viridis_d(option = &#34;viridis&#34;) + theme(legend.position = &#34;none&#34;) + xlab(&#34;&#34;) + ylab(&#34;Sentiment&#34;) Some routine checks When comparing results across different lexicons, consider word counts and the ratio of positive words to negative words. TODO: Update include and eval settings here - ok to keep in md doc but no need to knit them 1 2 3 get_sentiments(&#34;nrc&#34;) %&gt;% filter(sentiment %in% c(&#34;positive&#34;, &#34;negative&#34;)) %&gt;% count(sentiment) %&gt;% knitr::kable() sentiment n negative 3324 positive 2312 1 2 3 get_sentiments(&#34;bing&#34;) %&gt;% count(sentiment) %&gt;% knitr::kable() sentiment n negative 4781 positive 2005 1 2 3 4 5 6 bing_word_counts &lt;- tidy_books %&gt;% inner_join(get_sentiments(&#34;bing&#34;)) %&gt;% count(word, sentiment, sort = TRUE) %&gt;% ungroup() bing_word_counts %&gt;% head(10) %&gt;% knitr::kable() word sentiment n corruption negative 120 like positive 74 love positive 58 great positive 57 good positive 56 best positive 50 hurts negative 42 well positive 38 work positive 29 free positive 28 What are the top words driving sentiment score? 1 2 3 4 5 6 7 8 9 10 11 bing_word_counts %&gt;% group_by(sentiment) %&gt;% top_n(10) %&gt;% ungroup() %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(n, word, fill = sentiment)) + geom_col(show.legend = FALSE) + scale_fill_viridis_d(option = &#34;viridis&#34;) + facet_wrap(~sentiment, scales = &#34;free_y&#34;) + labs(x = &#34;Contribution to sentiment&#34;, y = NULL) Wordclouds I don't find wordclouds terrible useful, but they're not terrible for a background graphic or an attention grabber. First, let's add some custom stop words (i.e. words to exclude from the analysis). If you want to know why, run the next chunk and see how URLs (and obviously our original search terms) throw off our term frequency matrix. 1 2 3 4 5 custom_stop_words &lt;- bind_rows(tibble(word = c(&#39;philadelphia&#39;, &#39;philly&#39;, &#39;t.co&#39;, &#39;https&#39;, &#39;amp&#39;), lexicon = c(&#34;custom&#34;)), stop_words) custom_stop_words %&gt;% head() %&gt;% knitr::kable() word lexicon philadelphia custom philly custom t.co custom https custom amp custom a SMART 1 2 3 4 5 6 library(wordcloud) tidy_books %&gt;% anti_join(custom_stop_words) %&gt;% count(word) %&gt;% with(wordcloud(word, n, max.words = 100, col = brewer.pal(8, &#34;Dark2&#34;))) Going to compare the most popular positive and negative words using comparison.cloud(). Keep a look out for pronouns that get mistaken for positive or negative words (e.g. 'hurts' as in 'jalen hurts', though you might leave that in depending on how you feel about the Eagles qb situation). 1 2 3 4 5 6 7 8 library(reshape2) tidy_books %&gt;% inner_join(get_sentiments(&#34;bing&#34;)) %&gt;% count(word, sentiment, sort = TRUE) %&gt;% acast(word ~ sentiment, value.var = &#34;n&#34;, fill = 0) %&gt;% comparison.cloud(colors = c(&#34;orange&#34;, &#34;navy&#34;), max.words = 100) What about sentences? Rather than assign sentiment to words, some algorithms attempt to derive sentiment from complete sentences, e.g. &quot;I am not having a good day&quot; should imply negative sentiment, not positive. Our tweets are already pretty succinct, so we can try to look at the entire tweek to make an assessment. TODO: Fix how this is parsing sentences. See docs Using sentences, or some other type of index, we can assign a sentiment value - not by word - but by an entire chapter. Which chapter (date) had the highest proportion of positive words? 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 bingpositive &lt;- get_sentiments(&#34;bing&#34;) %&gt;% filter(sentiment == &#34;positive&#34;) wordcounts &lt;- tidy_books %&gt;% group_by(book, chapter) %&gt;% summarize(words = n()) tidy_books %&gt;% semi_join(bingpositive) %&gt;% group_by(book, chapter) %&gt;% summarize(positivewords = n()) %&gt;% left_join(wordcounts, by = c(&#34;book&#34;, &#34;chapter&#34;)) %&gt;% mutate(positive_ratio = positivewords/words) %&gt;% filter(chapter != 0) %&gt;% top_n(1) %&gt;% ungroup() ## # A tibble: 1 x 5 ## book chapter positivewords words positive_ratio ## &lt;chr&gt; &lt;date&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Philadelphia 2020-12-21 196 5755 0.0341 Acknowledgements This blog post was made possible thanks to: knitr rtweet (Kearney, 2019) rmarkdown NLP (Hornik, 2020) tm RColorBrewer (Neuwirth, 2014) wordcloud (Fellows, 2018) topicmodels (Gr√ºn and Hornik, 2011) SnowballC (Bouchet-Valat, 2020) config (Allaire, 2020) dplyr (Wickham, Fran√ßois, Henry, and M√ºller, 2020) tidytext (Silge and Robinson, 2016) tidyr (Wickham, 2020) ggplot2 (Wickham, 2016) lubridate (Grolemund and Wickham, 2011) readr (Wickham and Hester, 2020) skimr (Waring, Quinn, McNamara, Arino de la Rubia, et al., 2020) BiocStyle blogdown (Xie, Hill, and Thomas, 2017) devtools (Wickham, Hester, and Chang, 2020) knitcitations (Boettiger, 2020)]".toLowerCase(),},{"link":"/","title":"rjfranssen","body":"[Exploring geography and data sciences I created this site to organize my notes and personal GIS &amp; data science projects. This site includes code snippets, various shiny apps, and references to tutorials that I've found valuable. Thank you Yihui Xie, Amber Thomas, Alison Presmanes Hill for the wonderful Blogdown library &amp; book, Dan Weru for the clean Hugo theme &quot;Compose&quot;, and the RStudio folks for an awesome IDE. Explore the Posts This site's Repo Photo by Pixabay]".toLowerCase(),},];const searchKeys=['title','link','body','id'];const searchOptions={ignoreLocation:true,findAllMatches:true,includeScore:true,shouldSort:true,keys:searchKeys,threshold:0.0};const index=new Fuse(idx,searchOptions);function searchResults(results=[],query=""){let resultsFragment=new DocumentFragment();let showResults=elem('.search_results');emptyEl(showResults);if(results.length){let resultsTitle=createEl('h3');resultsTitle.className='search_title';resultsTitle.innerText='Quick Links';resultsFragment.appendChild(resultsTitle);results.slice(0,4).forEach(function(result){let item=createEl('a');item.href=`${result.link}?query=${query}`;item.className='search_result';item.textContent=result.title;item.style.order=result.score;resultsFragment.appendChild(item);});}else{showResults.innerHTML=(query.length>=3)?`<span class="search_result">No Results</span>`:"";}
showResults.appendChild(resultsFragment);}
function search(){const searchField=elem('.search_field');if(searchField){searchField.addEventListener('input',function(){const searchTerm=this.value.trim().toLowerCase();const isFloat=parseFloat(searchTerm);const minimumSearchTermLength=isFloat?2:3;if(searchTerm.length>=minimumSearchTermLength){let rawResults=index.search(searchTerm);rawResults=rawResults.map(function(result){const score=result.score;const resultItem=result.item;resultItem.score=(parseFloat(score)*50).toFixed(0);return resultItem;});searchResults(rawResults,searchTerm);}else{searchResults();}});}}
function findQuery(query='query'){const urlParams=new URLSearchParams(window.location.search);if(urlParams.has(query)){let c=urlParams.get(query);cc=`${c.charAt(0).toUpperCase()}${c.substring(1,c.length)}`;return[c,cc];}
return["",""];}
let main=elem('main');if(!main){main=elem('.main');}
const searchQuery=findQuery();wrapText(searchQuery[0],main);wrapText(searchQuery[1],main);window.addEventListener('load',()=>search());