, listed_count
, statuses_count
, created_at
, account_created_at
) %>%
head(2)
nrow(tweets_df) %>% print()
tweets_df %>%
slice_max(favorite_count, n = 1) %>% # can also use arrange()
select(screen_name
, text
, favorite_count
, created_at)
top_tweeters <- tweets_df %>%
group_by(screen_name) %>%
summarize(num_tweets = n()) %>%
arrange(-num_tweets) %>% # can also use slice_max()
ungroup() %>%
head(5)
ggplot(top_tweeters) +
geom_histogram(aes(x = screen_name, y = num_tweets, fill = screen_name), stat = 'identity') +
theme_minimal() +
scale_fill_viridis_d(option = "viridis") +
labs(title = "What users are tweeting the most?") +
theme(legend.position = "none") +
xlab("") +
ylab("Number of Tweets")
tweets_df %>%
group_by(lang) %>%
summarize(num_tweets = n()) %>%
arrange(-num_tweets) %>% # can also use slice_max()
head(5)
# ggplot(tweets_df) +
#   geom_bar(aes(x = lang, fill = lang))
users_by_source <- tweets_df %>%
group_by(source) %>%
summarize(num_users = n_distinct(user_id)) %>%
arrange(-num_users) %>% # can also use slice_max()
ungroup() %>%
head(10)
ggplot(users_by_source) +
geom_histogram(aes(x = reorder(source, num_users), y = num_users, fill = source), stat = 'identity') +
scale_fill_viridis_d(option = "viridis") +
coord_flip() +
theme_gray() +
theme(legend.position = "none") +
#theme(legend.title = element_blank()) +
labs(title = "Top 10 Twitter Platforms People are Using") +
xlab("") +
ylab("Number of Users") +
scale_y_continuous(breaks = seq(0, 450, by=50), limits = c(0, 450), expand = c(0, 0)) + # use expand() to remove empty space between axis and bars
geom_hline(yintercept = seq(0, 450, by = 25), color = 'white')
tweets_per_day <- tweets_df %>%
group_by(tweet_date) %>%
summarize(num_tweets = n()) %>%
arrange(-num_tweets) %>% # can also use slice_max()
ungroup()
# Using ggplot + plotly
library(plotly)
plt1 <- ggplot(tweets_per_day) +
geom_line(aes(x = tweet_date, y = num_tweets)) +
scale_fill_viridis_d(option = "viridis") +
theme_gray() +
theme(legend.position = "none") +
labs(title = "Daily Tweets for 'Philadelphia' (ggplot2 + plotly)") +
xlab("") +
ylab("Number of Tweets")
pltly1 <- plotly::ggplotly(plt1)
widget <- pltly1
widgetfile <- 'pltly1.html'
htmlwidgets::saveWidget(widget = widget
, selfcontained = TRUE
, file = widgetfile)
cat(paste0('<iframe src= "', widgetfile, '" width="100%" height="400" style="border: none;"></iframe>'))
# Using dygraph
# convert series to an xts object
tweets_ts <- xts::xts(tweets_per_day$num_tweets, order.by = tweets_per_day$tweet_date)
# Create the dygraph
library(dygraphs)
dygraph1 <- dygraph(tweets_ts, main = "Daily Tweets for 'Philadelphia' (dygraph)") %>%
dySeries("V1", label = "Tweet Count")
widget <- dygraph1
widgetfile <- 'dygraph1.html'
htmlwidgets::saveWidget(widget = widget
, selfcontained = TRUE
, file = widgetfile)
cat(paste0('<iframe src= "', widgetfile, '" width="100%" height="400" style="border: none;"></iframe>'))
library(leaflet)
tweet_map <- leaflet(tweets_df) %>%
addTiles() %>%
addMarkers(lng = ~lng
, lat = ~lat
, label = ~as.character(text)
, popup = ~as.character(text)
)
widget <- tweet_map
widgetfile <- 'tweet_map.html'
htmlwidgets::saveWidget(widget = widget
, selfcontained = TRUE
, file = widgetfile)
cat(paste0('<iframe src= "', widgetfile, '" width="100%" height="400" style="border: none;"></iframe>'))
#install.packages("textdata")
library(tidytext)
get_sentiments("afinn") %>% head()
get_sentiments("bing") %>% head()
get_sentiments("nrc") %>% head()
library(stringr)
library(janeaustenr)
tidy_books <- austen_books() %>%
group_by(book) %>%
mutate(
linenumber = row_number(),
chapter = cumsum(str_detect(text,
regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
tidy_books %>% head() %>% knitr::kable()
tweets_book <- cbind('Philadelphia', tweets_df$text) %>% data.frame()
colnames(tweets_book) <- c('book', 'text')
# Reformat the date - running above insists on converting the `tweet_date` value to its integer form.
tweets_book$chapter <- tweets_df$tweet_date
tidy_books <- tweets_book %>%
group_by(book) %>%
mutate(
linenumber = row_number()
) %>%
ungroup() %>%
unnest_tokens(word, text)
tidy_books %>% head() %>% knitr::kable()
nrc_joy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
head(nrc_joy)
tidy_books %>%
filter(book == "Philadelphia") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE) %>%
head(10)
philly_sentiment <- tidy_books %>%
inner_join(get_sentiments("bing"), by = c("word" = "word")) %>%
#count(book, index = linenumber %/% 80, sentiment) %>% # 80-line tweet chunks
count(book, index = chapter, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
philly_tweets <- tidy_books %>%
filter(book == "Philadelphia")
## afinn
afinn <- philly_tweets %>%
inner_join(get_sentiments("afinn"), by = c("word" = "word")) %>%
#group_by(index = linenumber %/% 80) %>% # Can use 80-line chunks
group_by(index = chapter) %>% # or other type of index value, like date
summarise(sentiment = sum(value)) %>%
mutate(method = "AFINN") %>%
ungroup()
## rbinding bing and nrc
bing_and_nrc <- bind_rows(
philly_tweets %>%
inner_join(get_sentiments("bing"), by = c("word" = "word")) %>%
mutate(method = "Bing et al."),
philly_tweets %>%
inner_join(get_sentiments("nrc") %>%
filter(sentiment %in% c("positive",
"negative"))
) %>%
mutate(method = "NRC")) %>%
#count(method, index = linenumber %/% 80, sentiment) %>% # Can use 80-line chunks
count(method, index = chapter, sentiment) %>% # or other type of index value, like date
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
bind_rows(afinn,
bing_and_nrc) %>%
ggplot(aes(index, sentiment, fill = method)) +
geom_col(show.legend = FALSE) +
facet_wrap(~method, ncol = 1, scales = "free_y") +
labs(title = "Sentiment of 'Philadelphia' Tweets - Lexicon Comparison") +
scale_fill_viridis_d(option = "viridis") +
theme(legend.position = "none") +
xlab("") +
ylab("Sentiment")
get_sentiments("nrc") %>%
filter(sentiment %in% c("positive", "negative")) %>%
count(sentiment)
get_sentiments("bing") %>%
count(sentiment)
bing_word_counts <- tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
bing_word_counts %>% head(10) %>% knitr::kable()
bing_word_counts %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_viridis_d(option = "viridis") +
facet_wrap(~sentiment, scales = "free_y") +
labs(x = "Contribution to sentiment",
y = NULL)
custom_stop_words <- bind_rows(tibble(word = c('philadelphia', 'philly', 't.co', 'https', 'amp'),
lexicon = c("custom")),
stop_words)
custom_stop_words %>% head() %>% knitr::kable()
blogdown:::serve_site()
# Knitr options: https://yihui.org/knitr/options/
library(lubridate)
knitr::opts_chunk$set(collapse = TRUE
, eval = TRUE
, echo = TRUE
, message = FALSE
, warning = FALSE
, include = TRUE)
tidy_books %>%
filter(book == "Philadelphia") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE) %>%
head(10)
blogdown:::serve_site()
build_site()
blogdown::build_site()
blogdown:::new_post_addin()
plt <- ggplot(mtcars) + geom_point(aes(cyl, mph))
library(ggplot2)
library(ggplot2)
library(dplyr)
library(plotly)
library(ggplot2)
library(dplyr)
library(plotly)
plt <- ggplot(mtcars) + geom_point(aes(cyl, mph))
library(ggplot2)
library(dplyr)
library(plotly)
plt <- ggplot(mtcars) + geom_point(aes(cyl, mph))
ggplotly(plotly)
plt <- ggplot(mtcars) + geom_point(aes(cyl, mph))
library(ggplot2)
library(dplyr)
library(plotly)
plt <- ggplot(mtcars) + geom_point(aes(cyl, mph))
ggplotly(plt)
library(ggplot2)
library(dplyr)
library(plotly)
plt <- ggplot(mtcars) + geom_point(aes(cyl, mpg))
ggplotly(plt)
blogdown:::serve_site()
blogdown:::serve_site()
# Knitr options: https://yihui.org/knitr/options/
library(lubridate)
library(ggplot2)
library(plotly)
library(dplyr)
knitr::opts_chunk$set(collapse = TRUE
, eval = TRUE
, echo = TRUE
, message = FALSE
, warning = FALSE
, include = TRUE)
## Load frequently used packages for blog posts
library('knitcitations') # for citations
library('BiocStyle') # for CRANpkg() Biocpkg() Githubpkg()
library('devtools') # for session_info()
## Load knitcitations with a clean bibliography
cleanbib()
cite_options(hyperlink = 'to.doc', citation_format = 'text', style = 'html')
bib <- c(
'rgdal' = citation('rgdal'),
'leaflet' = citation('leaflet'),
'dplyr' = citation('dplyr'),
'data.table' = citation('data.table'),
'htmlwidgets' = citation('htmlwidgets'),
'BiocStyle' = citation('BiocStyle'),
'blogdown' = citation('blogdown')[2],
'devtools' = citation('devtools'),
'knitcitations' = citation('knitcitations')
)
jupyter_data <- jsonlite::fromJSON('https://raw.githubusercontent.com/ageron/handson-ml2/master/05_support_vector_machines.ipynb')
nb_data <- jsonlite::fromJSON('https://raw.githubusercontent.com/ageron/handson-ml2/master/05_support_vector_machines.ipynb')
nb_file = tempfile(fileext = '.ipynb')
jsonlite::write_json(nb_data, nb_file, auto_unbox = TRUE, pretty = TRUE)
xfun::file_string(nb_file)
nb_rmd = rmarkdown:::convert_ipynb(nb_file)
xfun::file_string(nb_rmd)
write(nb_rmd, 'svm_notebook.Rmd')
write(xfun::file_string(nb_file), 'svm_notebook.Rmd')
write(xfun::file_string(nb_rmd), 'svm_notebook.Rmd')
# Knitr options: https://yihui.org/knitr/options/
library(dplyr)
library(ggplot2)
library(reticulate) # python
knitr::opts_chunk$set(collapse = TRUE
, eval = TRUE
, echo = TRUE
, message = FALSE
, warning = FALSE
, include = TRUE
, python = 'C:/Python38/python.exe'
#, ruby = '/usr/local/bin/ruby'
#, engine.opts = list(bash = "-l")
)
# Knitr options: https://yihui.org/knitr/options/
library(dplyr)
library(ggplot2)
library(reticulate) # python
knitr::opts_chunk$set(collapse = TRUE
, eval = TRUE
, echo = TRUE
, message = FALSE
, warning = FALSE
, include = TRUE
, python = 'C:/Python38/python.exe'
#, ruby = '/usr/local/bin/ruby'
#, engine.opts = list(bash = "-l")
)
# Knitr options: https://yihui.org/knitr/options/
library(dplyr)
library(ggplot2)
library(reticulate) # python
knitr::opts_chunk$set(collapse = TRUE
, eval = TRUE
, echo = TRUE
, message = FALSE
, warning = FALSE
, include = TRUE
, python = 'C:/Python38/python.exe'
#, ruby = '/usr/local/bin/ruby'
#, engine.opts = list(bash = "-l")
)
# Knitr options: https://yihui.org/knitr/options/
library(lubridate)
library(ggplot2)
library(plotly)
library(dplyr)
knitr::opts_chunk$set(collapse = TRUE
, eval = TRUE
, echo = TRUE
, message = FALSE
, warning = FALSE
, include = TRUE)
## Load frequently used packages for blog posts
library('knitcitations') # for citations
library('BiocStyle') # for CRANpkg() Biocpkg() Githubpkg()
library('devtools') # for session_info()
## Load knitcitations with a clean bibliography
cleanbib()
cite_options(hyperlink = 'to.doc', citation_format = 'text', style = 'html')
bib <- c(
'rgdal' = citation('rgdal'),
'leaflet' = citation('leaflet'),
'dplyr' = citation('dplyr'),
'data.table' = citation('data.table'),
'htmlwidgets' = citation('htmlwidgets'),
'BiocStyle' = citation('BiocStyle'),
'blogdown' = citation('blogdown')[2],
'devtools' = citation('devtools'),
'knitcitations' = citation('knitcitations')
)
# Knitr options: https://yihui.org/knitr/options/
library(dplyr)
library(ggplot2)
library(reticulate) # python
knitr::opts_chunk$set(collapse = TRUE
, eval = TRUE
, echo = TRUE
, message = FALSE
, warning = FALSE
, include = TRUE
, python = 'C:/Python38/python.exe'
#, ruby = '/usr/local/bin/ruby'
#, engine.opts = list(bash = "-l")
)
py$df
py$plt
py$X_outliers
# Knitr options: https://yihui.org/knitr/options/
library(dplyr)
library(ggplot2)
library(reticulate) # python
knitr::opts_chunk$set(collapse = TRUE
, eval = TRUE
, echo = TRUE
, message = FALSE
, warning = FALSE
, include = TRUE
, python = 'C:/Python38/python.exe'
#, ruby = '/usr/local/bin/ruby'
#, engine.opts = list(bash = "-l")
)
# Knitr options: https://yihui.org/knitr/options/
library(dplyr)
library(ggplot2)
library(reticulate) # python
library(JuliaCall) # julia
library(r2d3) # d3
knitr::opts_chunk$set(collapse = TRUE
, eval = TRUE
, echo = TRUE
, message = FALSE
, warning = FALSE
, include = TRUE
, python = 'C:/Python38/python.exe'
#, ruby = '/usr/local/bin/ruby'
, engine.opts = list(bash = "-l")
)
library(reticulate)
use_python("C:/Python38/python.exe")
matplotlib <- import("matplotlib")
library(reticulate)
use_python("C:/Python38/python.exe")
matplotlib <- import("matplotlib")
library(reticulate)
use_python("C:\Users\rober\anaconda3\envs\r-reticulate\python.exe")
library(reticulate)
use_python("C:/Users/rober/anaconda3/envs/r-reticulate/python.exe")
matplotlib <- import("matplotlib")
library(reticulate)
use_python("C:/Users/rober/anaconda3/envs/r-reticulate/python.exe")
matplotlib <- import("matplotlib")
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
# Knitr options: https://yihui.org/knitr/options/
library(lubridate)
library(ggplot2)
library(plotly)
library(dplyr)
knitr::opts_chunk$set(collapse = TRUE
, eval = TRUE
, echo = TRUE
, message = FALSE
, warning = FALSE
, include = TRUE)
## Load frequently used packages for blog posts
library('knitcitations') # for citations
library('BiocStyle') # for CRANpkg() Biocpkg() Githubpkg()
library('devtools') # for session_info()
## Load knitcitations with a clean bibliography
cleanbib()
cite_options(hyperlink = 'to.doc', citation_format = 'text', style = 'html')
bib <- c(
'rgdal' = citation('rgdal'),
'leaflet' = citation('leaflet'),
'dplyr' = citation('dplyr'),
'data.table' = citation('data.table'),
'htmlwidgets' = citation('htmlwidgets'),
'BiocStyle' = citation('BiocStyle'),
'blogdown' = citation('blogdown')[2],
'devtools' = citation('devtools'),
'knitcitations' = citation('knitcitations')
)
nb_data <- jsonlite::fromJSON('https://raw.githubusercontent.com/spmallick/learnopencv/master/KerasCNN-CIFAR/keras-cnn-cifar10.ipynb')
nb_file = tempfile(fileext = '.ipynb')
jsonlite::write_json(nb_data, nb_file, auto_unbox = TRUE, pretty = TRUE)
xfun::file_string(nb_file)
nb_rmd = rmarkdown:::convert_ipynb(nb_file)
```{r, eval = FALSE}
xfun::file_string(nb_rmd)
write(xfun::file_string(nb_rmd), 'keras-cnn-cifar10.Rmd')
# Knitr options: https://yihui.org/knitr/options/
library(dplyr)
library(ggplot2)
library(reticulate) # python
knitr::opts_chunk$set(collapse = TRUE
, eval = TRUE
, echo = TRUE
, message = FALSE
, warning = FALSE
, include = TRUE
, python = 'C:/Python38/python.exe'
)
knitr::opts_chunk$set(collapse = TRUE
, eval = TRUE
, echo = TRUE
, message = FALSE
, warning = FALSE
, include = TRUE
, python = 'C:/Python39/python.exe'
)
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
# Knitr options: https://yihui.org/knitr/options/
library(dplyr)
library(ggplot2)
library(tidyverse)
library(crypto)
install.packages("crypto")
# Knitr options: https://yihui.org/knitr/options/
library(dplyr)
library(ggplot2)
library(tidyverse)
library(crypto)
install.packages("crypto", dependencies = TRUE)
devtools::install_github("jessevent/crypto")
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
.libPaths()
install.packages("xfun")
install.packages("xfun")
blogdown:::serve_site()
.libPaths()
find -name Rprofile
