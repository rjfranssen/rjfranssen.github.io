geom_col(aes(index, sentiment, fill = sentiment)) +
#facet_wrap(~book, ncol = 1, scales = "free_x") # apply if we'relooking across multiple books +
labs(title = "Sentiment of 'Philadelphia' Tweets") +
scale_fill_gradient2(low='red', mid='orange', high='navy') +
#scale_fill_viridis_c(option = "viridis") +
theme(legend.position = "none") +
xlab("") +
ylab("Sentiment")
philly_tweets <- tidy_books %>%
filter(book == "Philadelphia")
## afinn
afinn <- philly_tweets %>%
inner_join(get_sentiments("afinn")) %>%
#group_by(index = linenumber %/% 80) %>% # Can use 80-line chunks
group_by(index = chapter) %>% # or other type of index value, like date
summarise(sentiment = sum(value)) %>%
mutate(method = "AFINN")
## rbinding bing and nrc
bing_and_nrc <- bind_rows(
philly_tweets %>%
inner_join(get_sentiments("bing")) %>%
mutate(method = "Bing et al."),
philly_tweets %>%
inner_join(get_sentiments("nrc") %>%
filter(sentiment %in% c("positive",
"negative"))
) %>%
mutate(method = "NRC")) %>%
#count(method, index = linenumber %/% 80, sentiment) %>% # Can use 80-line chunks
count(method, index = chapter, sentiment) %>% # or other type of index value, like date
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
bind_rows(afinn,
bing_and_nrc) %>%
ggplot(aes(index, sentiment, fill = method)) +
geom_col(show.legend = FALSE) +
facet_wrap(~method, ncol = 1, scales = "free_y") +
labs(title = "Sentiment of 'Philadelphia' Tweets - Lexicon Comparison") +
scale_fill_viridis_d(option = "viridis") +
theme(legend.position = "none") +
xlab("") +
ylab("Sentiment")
get_sentiments("nrc") %>%
filter(sentiment %in% c("positive", "negative")) %>%
count(sentiment) %>% knitr::kable()
get_sentiments("bing") %>%
count(sentiment) %>% knitr::kable()
bing_word_counts <- tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
bing_word_counts %>% head(10) %>% knitr::kable()
bing_word_counts %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
scale_fill_viridis_d(option = "viridis") +
facet_wrap(~sentiment, scales = "free_y") +
labs(x = "Contribution to sentiment",
y = NULL)
custom_stop_words <- bind_rows(tibble(word = c('philadelphia', 'philly', 't.co', 'https', 'amp'),
lexicon = c("custom")),
stop_words)
custom_stop_words %>% head() %>% knitr::kable()
library(wordcloud)
tidy_books %>%
anti_join(custom_stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100, col = brewer.pal(8, "Dark2")))
library(reshape2)
tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("orange", "navy"),
max.words = 100)
bingpositive <- get_sentiments("bing") %>%
filter(sentiment == "positive")
wordcounts <- tidy_books %>%
group_by(book, chapter) %>%
summarize(words = n())
tidy_books %>%
semi_join(bingpositive) %>%
group_by(book, chapter) %>%
summarize(positivewords = n()) %>%
left_join(wordcounts, by = c("book", "chapter")) %>%
mutate(positive_ratio = positivewords/words) %>%
filter(chapter != 0) %>%
top_n(1) %>%
ungroup()
## Load frequently used packages for blog posts
library('knitcitations') # for citations
library('BiocStyle') # for CRANpkg() Biocpkg() Githubpkg()
library('devtools') # for session_info()
## Load knitcitations with a clean bibliography
cleanbib()
cite_options(hyperlink = 'to.doc', citation_format = 'text', style = 'html')
bib <- c(
'knitr' = citation('knitr'),
'rtweet' = citation('rtweet'),
'rmarkdown' = citation('rmarkdown'),
'NLP' = citation('NLP'),
'tm' = citation('tm'),
'RColorBrewer' = citation('RColorBrewer'),
'wordcloud' = citation('wordcloud'),
'topicmodels' = citation('topicmodels'),
'SnowballC' = citation('SnowballC'),
'config' = citation('config'),
'dplyr' = citation('dplyr'),
'tidytext' = citation('tidytext'),
'tidyr' = citation('tidyr'),
'ggplot2' = citation('ggplot2'),
'lubridate' = citation('lubridate'),
'readr' = citation('readr'),
'skimr' = citation('skimr'),
'blogdown' = citation('blogdown')[2],
'devtools' = citation('devtools'),
'knitcitations' = citation('knitcitations')
)
blogdown:::serve_site()
# Knitr options: https://yihui.org/knitr/options/
library(dplyr)
library(ggplot2)
library(lubridate)
knitr::opts_chunk$set(collapse = TRUE
, eval = TRUE
, echo = TRUE
, message = FALSE
, warning = FALSE
, include = TRUE)
splashdown_colors <- c(
`ebony` = "#1E2225",
`blue_grotto` = "#05263B",
`charcoal` = "#43758A",
`light_sea_green` = "#36B4AF",
`pewter` = "#D3DADA",
`carafe` = "#55423A",
`goldenrod` = "#F1B416",
`burnt_sienna` = "#E56A19",
`crimson` = "#900000")
splashdown_cols <- function(...) {
cols <- c(...)
if (is.null(cols))
return (splashdown_colors)
splashdown_colors[cols]
}
library(scales)
library(dplyr)
par(mfrow=c(1,2))
splashdown_cols() %>% show_col()
splashdown_cols("light_sea_green", "burnt_sienna") %>% show_col()
ggplot(mtcars, aes(hp, mpg)) +
geom_point(color = splashdown_cols("goldenrod"),
size = 4, alpha = .8)
splashdown_palettes <- list(
`primary`  = splashdown_cols("ebony", "blue_grotto", "charcoal", "light_sea_green", "pewter"),
`secondary`  = splashdown_cols("carafe", "goldenrod", "burnt_sienna", "crimson"),
`all`   = splashdown_cols("ebony", "blue_grotto", "charcoal", "light_sea_green", "pewter", "carafe", "goldenrod", "burnt_sienna", "crimson")
)
splashdown_pal <- function(palette = "main", reverse = FALSE, ...) {
pal <- splashdown_palettes[[palette]]
if (reverse) pal <- rev(pal)
colorRampPalette(pal, ...)
}
par(mfrow=c(1,2))
splashdown_pal("primary")(12) %>% show_col()
splashdown_pal("secondary")(4) %>% show_col()
scale_color_rjfranssen <- function(palette = "main",
discrete = TRUE,
reverse = FALSE,
...) {
pal <- splashdown_pal(palette = palette, reverse = reverse)
if (discrete) {
discrete_scale("colour", paste0("splashdown_", palette), palette = pal, ...)
} else {
scale_color_gradientn(colours = pal(256), ...)
}
}
scale_fill_rjfranssen <- function(palette = "main", discrete = TRUE, reverse = FALSE, ...) {
pal <- splashdown_pal(palette = palette, reverse = reverse)
if (discrete) {
discrete_scale("fill", paste0("splashdown_", palette), palette = pal, ...)
} else {
scale_fill_gradientn(colours = pal(256), ...)
}
}
ggplot(iris, aes(Sepal.Width, Sepal.Length, color = Sepal.Length)) +
geom_point(size = 4, alpha = .6) +
scale_color_rjfranssen(discrete = FALSE, palette = "primary")
ggplot(iris, aes(Sepal.Width, Sepal.Length, color = Species)) +
geom_point(size = 4) +
scale_color_rjfranssen("secondary")
ggplot(mpg, aes(manufacturer, fill = manufacturer)) +
geom_bar() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
scale_fill_rjfranssen(palette = "all", guide = "none")
## Load frequently used packages for blog posts
library('knitcitations') # for citations
library('BiocStyle') # for CRANpkg() Biocpkg() Githubpkg()
library('devtools') # for session_info()
## Load knitcitations with a clean bibliography
cleanbib()
cite_options(hyperlink = 'to.doc', citation_format = 'text', style = 'html')
bib <- c(
'lubridate' = citation('lubridate'),
'dplyr' = citation('dplyr'),
'ggplot2' = citation('ggplot2'),
'BiocStyle' = citation('BiocStyle'),
'blogdown' = citation('blogdown')[2],
'devtools' = citation('devtools'),
'knitcitations' = citation('knitcitations')
)
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::insert_image_addin()
# Knitr options: https://yihui.org/knitr/options/
library(lubridate)
knitr::opts_chunk$set(collapse = TRUE
, eval = TRUE
, echo = TRUE
, message = FALSE
, warning = FALSE
, include = TRUE)
## Load frequently used packages for blog posts
library('knitcitations') # for citations
library('BiocStyle') # for CRANpkg() Biocpkg() Githubpkg()
library('devtools') # for session_info()
## Load knitcitations with a clean bibliography
cleanbib()
cite_options(hyperlink = 'to.doc', citation_format = 'text', style = 'html')
bib <- c(
'BiocStyle' = citation('BiocStyle'),
'blogdown' = citation('blogdown')[2],
'devtools' = citation('devtools'),
'knitcitations' = citation('knitcitations')
)
# Knitr options: https://yihui.org/knitr/options/
library(lubridate)
knitr::opts_chunk$set(collapse = TRUE
, eval = TRUE
, echo = TRUE
, message = FALSE
, warning = FALSE
, include = TRUE)
## Load frequently used packages for blog posts
library('knitcitations') # for citations
library('BiocStyle') # for CRANpkg() Biocpkg() Githubpkg()
library('devtools') # for session_info()
## Load knitcitations with a clean bibliography
cleanbib()
cite_options(hyperlink = 'to.doc', citation_format = 'text', style = 'html')
bib <- c(
'rgdal' = citation('rgdal'),
'leaflet' = citation('leaflet'),
'dplyr' = citation('dplyr'),
'data.table' = citation('data.table'),
'htmlwidgets' = citation('htmlwidgets'),
'BiocStyle' = citation('BiocStyle'),
'blogdown' = citation('blogdown')[2],
'devtools' = citation('devtools'),
'knitcitations' = citation('knitcitations')
)
library(rgdal)
library(leaflet)
library(dplyr)
library(data.table)
library(htmlwidgets)
bib
blogdown:::insert_image_addin()
zcta_shapefile <-  rgdal::readOGR(
dsn = "cb_2018_us_zcta510_500k", # this is the unzipped folder
layer = "cb_2018_us_zcta510_500k", # this is the file inside of the unzipped folder
verbose = FALSE
)
# identify Alabama zip codes
al_zips <- 35004:36925
# get some random numbers
sales <- runif(length(al_zips), min=1000, max=5000) %>% round(0)
# put this in a dataframe
sales_data <- data.frame(
state = 'AL',
zip = al_zips,
sales = sales
)
# ok, now i have some fake sales numbers
sales_data %>% head(5)
# copy data
zcta_data <- data.table::copy(zcta_shapefile@data)
#  join sales data - it's very important that we do NOT  duplicate or drop rows or it's going to screw up the shapefile
zcta_data$ZCTA5CE10 <- as.numeric(zcta_data$ZCTA5CE10)
zcta_data <- zcta_data %>% left_join(sales_data, by = c("ZCTA5CE10" = "zip"))
# Now reattach this data file back to the SpatialPolygonsDataFrame data slot
zcta_shapefile@data <- zcta_data
# (optional) There are 33k ZCTAs in the US; consider reducing these to a particular region of interest.
# I'm using the `state` value that i just joined for demo purposes. You can skip this if you want to do the whole country
zcta_shapefile <- zcta_shapefile[!is.na(zcta_shapefile@data$state) & zcta_shapefile@data$state=='AL',]
nrow(zcta_shapefile@data)
length(zcta_shapefile@polygons)
labels <- sprintf(
"<strong>Zip: %s</strong><br/> Sales: $%s",
zcta_shapefile@data$ZCTA5CE10,
prettyNum(zcta_shapefile@data$sales, big.mark=",")
) %>% lapply(htmltools::HTML)
# copy data
zcta_data <- data.table::copy(zcta_shapefile@data)
#  join sales data - it's very important that we do NOT  duplicate or drop rows or it's going to screw up the shapefile
zcta_data$ZCTA5CE10 <- as.numeric(zcta_data$ZCTA5CE10)
zcta_data <- zcta_data %>% left_join(sales_data, by = c("ZCTA5CE10" = "zip"))
# Now reattach this data file back to the SpatialPolygonsDataFrame data slot
zcta_shapefile@data <- zcta_data
# (optional) There are 33k ZCTAs in the US; consider reducing these to a particular region of interest.
# I'm using the `state` value that i just joined for demo purposes. You can skip this if you want to do the whole country
zcta_shapefile <- zcta_shapefile[!is.na(zcta_shapefile@data$state) & zcta_shapefile@data$state=='AL',]
nrow(zcta_shapefile@data)
length(zcta_shapefile@polygons)
zcta_shapefile <-  rgdal::readOGR(
dsn = "cb_2018_us_zcta510_500k", # this is the unzipped folder
layer = "cb_2018_us_zcta510_500k", # this is the file inside of the unzipped folder
verbose = FALSE
)
# identify Alabama zip codes
al_zips <- 35004:36925
# get some random numbers
sales <- runif(length(al_zips), min=1000, max=5000) %>% round(0)
# put this in a dataframe
sales_data <- data.frame(
state = 'AL',
zip = al_zips,
sales = sales
)
# ok, now i have some fake sales numbers
sales_data %>% head(5)
# copy data
zcta_data <- data.table::copy(zcta_shapefile@data)
#  join sales data - it's very important that we do NOT  duplicate or drop rows or it's going to screw up the shapefile
zcta_data$ZCTA5CE10 <- as.numeric(zcta_data$ZCTA5CE10)
zcta_data <- zcta_data %>% left_join(sales_data, by = c("ZCTA5CE10" = "zip"))
# Now reattach this data file back to the SpatialPolygonsDataFrame data slot
zcta_shapefile@data <- zcta_data
# (optional) There are 33k ZCTAs in the US; consider reducing these to a particular region of interest.
# I'm using the `state` value that i just joined for demo purposes. You can skip this if you want to do the whole country
zcta_shapefile <- zcta_shapefile[!is.na(zcta_shapefile@data$state) & zcta_shapefile@data$state=='AL',]
nrow(zcta_shapefile@data)
length(zcta_shapefile@polygons)
nrow(zcta_shapefile@data) == length(zcta_shapefile@polygons)
nrow(zcta_shapefile@data)
length(zcta_shapefile@polygons)
nrow(zcta_shapefile@data) == length(zcta_shapefile@polygons)
nrow(zcta_shapefile@data)
length(zcta_shapefile@polygons)
labels <- sprintf(
"<strong>Zip: %s</strong><br/> Sales: $%s",
zcta_shapefile@data$ZCTA5CE10,
prettyNum(zcta_shapefile@data$sales, big.mark=",")
) %>% lapply(htmltools::HTML)
map_pal <-  leaflet::colorNumeric(palette="viridis", domain = zcta_shapefile@data$sales, na.color="transparent")
widget <- zcta_map
zcta_map <- zcta_shapefile %>%
leaflet::leaflet(options = leafletOptions(preferCanvas = TRUE)) %>%
# Check out map providers here: https://leaflet-extras.github.io/leaflet-providers/preview/
addProviderTiles(providers$Esri.WorldStreetMap, options = providerTileOptions(
updateWhenZooming = FALSE,      # map won't update tiles until zoom is done
updateWhenIdle = TRUE           # map won't load new tiles when panning
)) %>%
# Alabama zip 35148
setView(lat = 33.7492174, lng = -87.0823462, zoom = 7) %>%
# Now add those polygons!
addPolygons(
fillColor = ~map_pal(sales), # the map palette we made
weight = 2,
opacity = 1,
color = "white",
dashArray = "3",
stroke = TRUE,
fillOpacity = 0.5,
highlight = highlightOptions(
weight = 5,
color = "#667",
dashArray = "",
fillOpacity = 0.7,
bringToFront = TRUE
),
label = labels, # cool labels
labelOptions = labelOptions(
style = list("font-weight" = "normal", padding = "3px 8px"),
textsize = "15px",
directon = "auto"
)
) %>%
# Dont forget a legend
addLegend(
pal = map_pal,
values =  ~sales,
opacity = 0.7,
title = "Sales"
)
widget <- zcta_map
widgetfile <- 'zcta_map.html'
htmlwidgets::saveWidget(widget = widget
, selfcontained = TRUE
, file = widgetfile)
cat(paste0('<iframe src= "', widgetfile, '" width="100%" height="400" style="border: none;"></iframe>'))
# Knitr options: https://yihui.org/knitr/options/
library(lubridate)
knitr::opts_chunk$set(collapse = TRUE
, eval = TRUE
, echo = TRUE
, message = FALSE
, warning = FALSE
, include = TRUE)
tweets_df <- readRDS("tweets_df.Rds")
tweets_df %>% head()
#summary(tweet_df)
skim(tweets_df)
library(rtweet)
library(rmarkdown)
library(NLP)
library(tm)
library(RColorBrewer)
library(wordcloud)
library(topicmodels)
library(SnowballC)
library(config)
library(dplyr)
library(tidytext)
library(tidyr)
library(ggplot2)
library(lubridate)
library(readr)
library(skimr)
library(rtweet)
library(rmarkdown)
library(NLP)
library(tm)
library(RColorBrewer)
library(wordcloud)
library(topicmodels)
library(SnowballC)
library(config)
library(dplyr)
library(tidytext)
library(tidyr)
library(ggplot2)
library(lubridate)
library(readr)
library(skimr)
nrow(tweets_df) %>% print()
library(leaflet)
tweet_map <- leaflet(tweets_df) %>%
addTiles() %>%
addCircleMarkers(lng = ~lng
, lat = ~lat)
tweet_map
tweet_map <- leaflet(tweets_df) %>%
addTiles() %>%
addCircleMarkers(lng = ~lng
, lat = ~lat
, popup = ~as.character(text)
)
tweet_map
library(leaflet)
tweet_map <- leaflet(tweets_df) %>%
addTiles() %>%
addCircleMarkers(lng = ~lng
, lat = ~lat
, popup = ~as.character(text)
, label = ~as.character(text)
)
tweet_map
library(leaflet)
tweet_map <- leaflet(tweets_df) %>%
addTiles() %>%
addMarkers(lng = ~lng
, lat = ~lat
, label = ~as.character(screen_name)
, popup = ~as.character(text)
)
tweet_map
widget <- tweet_map
widgetfile <- 'tweet_map.html'
htmlwidgets::saveWidget(widget = widget
, selfcontained = TRUE
, file = widgetfile)
cat(paste0('<iframe src= "', widgetfile, '" width="100%" height="400" style="border: none;"></iframe>'))
widget <- tweet_map
widgetfile <- 'tweet_map.html'
htmlwidgets::saveWidget(widget = widget
, selfcontained = TRUE
, file = widgetfile)
cat(paste0('<iframe src= "', widgetfile, '" width="100%" height="400" style="border: none;"></iframe>'))
tweet_map
library(leaflet)
tweet_map <- leaflet(tweets_df) %>%
addTiles() %>%
addMarkers(lng = ~lng
, lat = ~lat
, label = ~as.character(text)
, popup = ~as.character(text)
)
widget <- tweet_map
widgetfile <- 'tweet_map.html'
htmlwidgets::saveWidget(widget = widget
, selfcontained = TRUE
, file = widgetfile)
cat(paste0('<iframe src= "', widgetfile, '" width="100%" height="400" style="border: none;"></iframe>'))
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
