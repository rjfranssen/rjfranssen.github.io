geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
library(ggplot2)
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered() +
labs(
title = 'Top Terms in Twitter Corpus'
, caption = 'The terms that are most common within each topic'
)
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered() +
labs(
title = 'Top Terms in Twitter Corpus'
, subtitle = 'The terms that are most common within each topic'
)
library(tidyr)
beta_spread <- topics %>%
mutate(topic = paste0("topic", topic)) %>%
spread(topic, beta) %>%
filter(topic1 > .001 | topic2 > .001) %>%
mutate(log_ratio = log2(topic2 / topic1))
beta_spread
documents <- tidy(lda, matrix = "gamma")
documents
documents %>% arrange(-gamma) %>% head(50)
documents %>% arrange(gamma) %>% head(50)
documents %>% arrange(document) %>% head(50)
documents %>% arrange(topic) %>% head(50)
tidy(corpus) %>%
filter(document == 6) %>%
arrange(desc(count))
tidy(tweets) %>%
filter(document == 6) %>%
arrange(desc(count))
head(tweets)
str(tweets)
str(dtm)
dtm$docs
dtm$Docs
dtm$dimnames$Docs
tidy(dtm$dimnames) %>%
filter(Docs == 1) %>%
arrange(desc(count))
dtm$dimnames %>%
filter(Docs == 1) %>%
arrange(desc(count))
filter(dtm$dimnames == 1) %>%
arrange(desc(count))
filter(dtm$dimnames$Docs == 1) %>%
arrange(desc(count))
dtm$dimnames$Docs
SEED = sample(1:1000000, 1)  # Pick a random seed for replication
k = 10  # Let's start with 10 topics
# This might take a minute!
models <- list(
CTM       = CTM(dtm, k = k, control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3))),
VEM       = LDA(dtm, k = k, control = list(seed = SEED)),
VEM_Fixed = LDA(dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),
Gibbs     = LDA(dtm, k = k, method = "Gibbs", control = list(seed = SEED, burnin = 1000,
thin = 100,    iter = 1000))
)
lapply(models, terms, 10)
assignments <- sapply(models, topics)
View(assignments)
#tweets <- tweet_corpus("energy+electricity", n = 100, lang = 'en')
tweets <- tweet_corpus('from:GrittyNHL', n = 1000, lang = 'en')
tweet_corpus <- function(search, n = 5000, ...) {
payload <- searchTwitter(search, n = n, ...)
sapply(payload, tweet_text)
}
#tweets <- tweet_corpus("energy+electricity", n = 100, lang = 'en')
tweets <- tweet_corpus('from:GrittyNHL', n = 1000, lang = 'en')
tweets <- iconv(tweets, to = "ASCII", sub = " ")  # Convert to basic ASCII text
tweets <- tolower(tweets)  # Make everything lower case
tweets <- gsub("rt", " ", tweets)  # Remove the "RT" (retweet) so duplicates are duplicates
tweets <- gsub("@\\w+", " ", tweets)  # Remove user names
tweets <- gsub("http.+ |http.+$", " ", tweets)  # Remove links
tweets <- gsub("[[:punct:]]", " ", tweets)  # Remove punctuation
tweets <- gsub("[ |\t]{2,}", " ", tweets)  # Remove tabs
tweets <- gsub("amp", " ", tweets)  # "&" is "&amp" in HTML, so after punctuation removed ...
tweets <- gsub("^ ", "", tweets)  # Leading blanks
tweets <- gsub(" $", "", tweets)  # Lagging blanks
tweets <- gsub(" +", " ", tweets) # General spaces (should just do all whitespaces no?)
tweets <- unique(tweets)  # Now get rid of duplicates!
corpus <- tm::Corpus(VectorSource(tweets))
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, removeNumbers, mc.cores=1)
corpus <- tm_map(corpus, stemDocument)
pal <- brewer.pal(8, "Dark2")
wordcloud(corpus, min.freq=2, max.words = 150, random.order = TRUE, col = pal)
#tweets <- tweet_corpus("energy+electricity", n = 100, lang = 'en')
tweets <- tweet_corpus('gritty', n = 1000, lang = 'en')
tweets <- iconv(tweets, to = "ASCII", sub = " ")  # Convert to basic ASCII text
tweets <- tolower(tweets)  # Make everything lower case
tweets <- gsub("rt", " ", tweets)  # Remove the "RT" (retweet) so duplicates are duplicates
tweets <- gsub("@\\w+", " ", tweets)  # Remove user names
tweets <- gsub("http.+ |http.+$", " ", tweets)  # Remove links
tweets <- gsub("[[:punct:]]", " ", tweets)  # Remove punctuation
tweets <- gsub("[ |\t]{2,}", " ", tweets)  # Remove tabs
tweets <- gsub("amp", " ", tweets)  # "&" is "&amp" in HTML, so after punctuation removed ...
tweets <- gsub("^ ", "", tweets)  # Leading blanks
tweets <- gsub(" $", "", tweets)  # Lagging blanks
tweets <- gsub(" +", " ", tweets) # General spaces (should just do all whitespaces no?)
tweets <- unique(tweets)  # Now get rid of duplicates!
corpus <- tm::Corpus(VectorSource(tweets))
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, removeNumbers, mc.cores=1)
corpus <- tm_map(corpus, stemDocument)
pal <- brewer.pal(8, "Dark2")
wordcloud(corpus, min.freq=2, max.words = 150, random.order = TRUE, col = pal)
#corpus <- tm_map(corpus, removeWords, c("energy", "electricity"))
corpus <- tm_map(corpus, removeWords, c("gritty"))
corpus <- tm::Corpus(VectorSource(tweets))
corpus <- tm_map(corpus, removeWords, stopwords("en"))
#corpus <- tm_map(corpus, removeWords, c("energy", "electricity"))
corpus <- tm_map(corpus, removeWords, c("gritty"))
corpus <- tm_map(corpus, removeNumbers, mc.cores=1)
corpus <- tm_map(corpus, stemDocument)
pal <- brewer.pal(8, "Dark2")
wordcloud(corpus, min.freq=2, max.words = 150, random.order = TRUE, col = pal)
#tweets <- tweet_corpus("energy+electricity", n = 100, lang = 'en')
tweets <- tweet_corpus('tesla+tsla', n = 1000, lang = 'en')
tweets <- iconv(tweets, to = "ASCII", sub = " ")  # Convert to basic ASCII text
tweets <- tolower(tweets)  # Make everything lower case
tweets <- gsub("rt", " ", tweets)  # Remove the "RT" (retweet) so duplicates are duplicates
tweets <- gsub("@\\w+", " ", tweets)  # Remove user names
tweets <- gsub("http.+ |http.+$", " ", tweets)  # Remove links
tweets <- gsub("[[:punct:]]", " ", tweets)  # Remove punctuation
tweets <- gsub("[ |\t]{2,}", " ", tweets)  # Remove tabs
tweets <- gsub("amp", " ", tweets)  # "&" is "&amp" in HTML, so after punctuation removed ...
tweets <- gsub("^ ", "", tweets)  # Leading blanks
tweets <- gsub(" $", "", tweets)  # Lagging blanks
tweets <- gsub(" +", " ", tweets) # General spaces (should just do all whitespaces no?)
tweets <- unique(tweets)  # Now get rid of duplicates!
corpus <- tm::Corpus(VectorSource(tweets))
corpus <- tm_map(corpus, removeWords, stopwords("en"))
#corpus <- tm_map(corpus, removeWords, c("energy", "electricity"))
corpus <- tm_map(corpus, removeWords, c("gritty"))
corpus <- tm_map(corpus, removeNumbers, mc.cores=1)
corpus <- tm_map(corpus, stemDocument)
#corpus <- tm_map(corpus, removeWords, c("energy", "electricity"))
corpus <- tm_map(corpus, removeWords, c('tesla', 'tsla'))
corpus <- tm_map(corpus, removeNumbers, mc.cores=1)
corpus <- tm_map(corpus, stemDocument)
pal <- brewer.pal(8, "Dark2")
wordcloud(corpus, min.freq=2, max.words = 150, random.order = TRUE, col = pal)
doc_lengths <- corpus %>% DocumentTermMatrix() %>% as.matrix() %>% rowSums()
dtm <- corpus[doc_lengths > 0] %>% DocumentTermMatrix()
lda <- LDA(x = dtm, k = 2, method = 'VEM', control = list(seed = 1234))
topics <- tidy(lda, matrix = "beta")
topics %>% arrange(-beta) %>% head(50)
top_terms <- topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered() +
labs(
title = 'Top Terms in Twitter Corpus'
, subtitle = 'The terms that are most common within each topic'
)
beta_spread <- topics %>%
mutate(topic = paste0("topic", topic)) %>%
spread(topic, beta) %>%
filter(topic1 > .001 | topic2 > .001) %>%
mutate(log_ratio = log2(topic2 / topic1))
beta_spread
documents <- tidy(lda, matrix = "gamma")
documents %>% arrange(topic) %>% head(50)
documents %>% arrange(document) %>% head(50)
saveRDS(tweets, file = "tweets.Rds", compress = 'xz')
tweets <- readRDS("tweets.Rds")
get_sentiments("afinn")
library(tidytext)
get_sentiments("afinn")
install.packages("textdata")
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
#install.packages("textdata")
library(tidytext)
get_sentiments("afinn") %>% head()
get_sentiments("bing") %>% head()
get_sentiments("nrc") %>% head()
library(stringr)
library(janeaustenr)
library(stringr)
tidy_books <- austen_books() %>%
group_by(book) %>%
mutate(
linenumber = row_number(),
chapter = cumsum(str_detect(text,
regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
View(tidy_books)
str(tidy_books)
str(austen_books())
austen_books()
austen_books() %>% View()
library(stringr)
tidy_books <- austen_books() %>%
group_by(book) %>%
mutate(
linenumber = row_number(),
chapter = cumsum(str_detect(text,
regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
tidy_books %>% head()
#tweets <- tweet_corpus("energy+electricity", n = 100, lang = 'en')
tweets <- tweet_corpus('philadelphia+philly', n = 1000, lang = 'en')
saveRDS(tweets, file = "tweets.Rds", compress = 'xz')
tweets <- readRDS("tweets.Rds")
tweets %>% head()
tweets %>% head(5)
str(tweets)
nrow(tweets)
length(tweets)
nrow(data.frame(tweets))
tweets_df <- data.frame(tweets)
?unnest_tokens
austen_books() %>%
group_by(book) %>%
mutate(
linenumber = row_number(),
chapter = cumsum(str_detect(text,
regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
tweets_df <- data.frame(tweets)
tweets_tbl <- tweets_df %>% as_tibble(book = 'Philadelphia'
, line_number = row_number()
, chapter = 'Tweets'
, )
tidy_books <- tweets_tbl %>%
group_by(book) %>%
mutate(
linenumber = line_number,
chapter = chapter) %>%
ungroup() %>%
unnest_tokens(word, text)
tweets_tbl <- tweets_df %>% as_tibble(book = 'Philadelphia'
, line_number = row_number()
, chapter = 'Tweets'
, word = tweets )
tidy_books <- tweets_tbl %>%
group_by(book) %>%
mutate(
linenumber = line_number,
chapter = chapter) %>%
ungroup() %>%
unnest_tokens(word, text)
tweets_tbl <- tweets_df %>% as_tibble(book = 'Philadelphia'
, line_number = row_number()
, chapter = 'Tweets'
, word = tweets )
tweets_tbl <- tweets_df %>% data.frame(book = 'Philadelphia'
, line_number = row_number()
, chapter = 'Tweets'
, word = tweets )
tweets_tbl <- tweets_df %>% data.frame(book = 'Philadelphia'
#, line_number = row_number()
, chapter = 'Tweets'
, word = tweets )
tweets_df <- data.frame(tweets)
tweets_tbl <- tweets_df %>% data.frame(book = 'Philadelphia'
#, line_number = row_number()
, chapter = 'Tweets'
, word = tweets )
tidy_books <- tweets_tbl %>%
group_by(book) %>%
mutate(
linenumber = row_number(),
chapter = chapter) %>%
ungroup() %>%
unnest_tokens(word, text)
tweets_tbl <- tweets_df %>% as_tibble(book = 'Philadelphia'
#, line_number = row_number()
, chapter = 'Tweets'
, word = tweets )
tweets_df <- data.frame(book = 'Philadelphia'
, chapter = 'Tweets'
, word = tweets_df$tweets)
View(tweets_df)
tidy_books <- tweets_df %>%
group_by(book) %>%
mutate(
linenumber = row_number(),
chapter = chapter) %>%
ungroup() %>%
unnest_tokens(word, text)
tidy_books <- as_tibble(tweets_df) %>%
group_by(book) %>%
mutate(
linenumber = row_number(),
chapter = chapter) %>%
ungroup() %>%
unnest_tokens(word, text)
tidy_books <- tweets_df %>%
group_by(book) %>%
mutate(
linenumber = row_number()
) %>%
ungroup() %>%
unnest_tokens(word, text)
library(readr)
searchTwitter('from:GrittyNHL', resultType = "popular", n = 50)
## Using resultType
searchTwitter('philadelphia+philly', resultType = "recent", n = 15)
head(phl_tweets)
phl_tweets <- searchTwitter('philadelphia+philly', resultType = "recent", n = 15)
head(phl_tweets)
str(phl_tweets)
head(phl_tweets, 1) %>% str()
getText()
install.packages("rtweet")
library(rtweet)
## install remotes package if it's not already
if (!requireNamespace("remotes", quietly = TRUE)) {
install.packages("remotes")
}
## install dev version of rtweet from github
remotes::install_github("ropensci/rtweet")
## load rtweet package
library(rtweet)
## search for 18000 tweets using the rstats hashtag
rt <- search_tweets(
"#rstats", n = 500, include_rts = FALSE
)
vignette("auth", package = "rtweet")
config <- config::get(file = 'config.yml')
## authenticate via web browser
token <- create_token(
app = config$app,
consumer_key = config$apikey,
consumer_secret = config$apikeysecret,
access_token = config$accesstoken,
access_secret = config$accesstokensecret)
token
## search for 18000 tweets using the rstats hashtag
rt <- search_tweets(
"#rstats", n = 500, include_rts = FALSE
)
?search_tweets
rt %>%
ts_plot("3 hours") +
ggplot2::theme_minimal() +
ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of #rstats Twitter statuses from past 9 days",
subtitle = "Twitter status (tweet) counts aggregated using three-hour intervals",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
## search for 18000 tweets using the rstats hashtag
rt <- search_tweets(
query = 'philadelphia philly'
, n = 500
, include_rts = FALSE
)
## search for 18000 tweets using the rstats hashtag
rt <- search_tweets(
q = 'philadelphia philly'
, n = 500
, include_rts = FALSE
)
rt %>%
ts_plot("3 hours") +
ggplot2::theme_minimal() +
ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of #rstats Twitter statuses from past 9 days",
subtitle = "Twitter status (tweet) counts aggregated using three-hour intervals",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
rt %>%
ts_plot("1 hours") +
ggplot2::theme_minimal() +
ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of #rstats Twitter statuses from past 9 days",
subtitle = "Twitter status (tweet) counts aggregated using three-hour intervals",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
## search for 18000 tweets using the rstats hashtag
rt <- search_tweets(
q = 'philadelphia philly'
, n = 500
, include_rts = FALSE
, type = 'recent'
#, geocode = "37.78, -122.40, 1mi"
#, retryonratelimit = TRUE
#, parse = FALSE
#, lang = 'en'
# ... see Twitter's REST API for more search parameters
)
rt %>%
ts_plot('1 hours') +
ggplot2::theme_minimal() +
ggplot2::theme(plot.title = ggplot2::element_text(face = 'bold')) +
ggplot2::labs(
x = NULL, y = NULL,
title = 'Frequency of "philadelphia" Twitter statuses from past 9 days',
subtitle = 'Twitter status (tweet) counts aggregated using three-hour intervals',
caption = '\nSource: Data collected from Twitters REST API via rtweet'
)
?lat_lng()
## create lat/lng variables using all available tweet and profile geo-location data
rt <- lat_lng(rt)
str(rt)
## plot state boundaries
par(mar = c(0, 0, 0, 0))
maps::map("state", lwd = .25)
## plot lat and lng points onto state map
with(rt, points(lng, lat, pch = 20, cex = .75, col = rgb(0, .3, .7, .75)))
rt$lat
rt %>%
ts_plot('1 hours') +
ggplot2::theme_minimal() +
ggplot2::theme(plot.title = ggplot2::element_text(face = 'bold')) +
ggplot2::labs(
x = NULL, y = NULL,
title = 'Frequency of "philadelphia" Twitter statuses from past 9 days',
subtitle = 'Twitter status (tweet) counts aggregated using three-hour intervals',
caption = '\nSource: Data collected from Twitters REST API via rtweet'
)
rt$lat
## create lat/lng variables using all available tweet and profile geo-location data
rt <- lat_lng(rt)
leaflet(rt) %>%
addTiles()
library(leaflet)
leaflet(rt) %>%
addTiles()
rt_geo <- rt %>% filter(!is.na(lat) $ !ia.na(long))
rt_geo <- rt %>% filter(!is.na(lat))
View(rt_geo)
rt_geo$lat
rt_geo$lng
rt_geo <- rt %>% filter(!is.na(lat) $ !is.na(lng))
rt_geo <- rt %>% filter(!is.na(lat) & !is.na(lng))
leaflet(rt_geo) %>%
addTiles()
leaflet(rt_geo) %>%
addTiles() %>%
addCircleMarkers(lng = ~lng
, lat = ~lat)
## search for 18000 tweets using the rstats hashtag
rt <- search_tweets(
q = 'philadelphia philly'
, n = 500
, include_rts = FALSE
, type = 'recent'
#, geocode = "37.78, -122.40, 1mi"
#, retryonratelimit = TRUE
#, parse = FALSE
#, lang = 'en'
# ... see Twitter's REST API for more search parameters
)
rt %>%
ts_plot('1 hours') +
ggplot2::theme_minimal() +
ggplot2::theme(plot.title = ggplot2::element_text(face = 'bold')) +
ggplot2::labs(
x = NULL, y = NULL,
title = 'Frequency of "philadelphia" Twitter statuses from past 9 days',
subtitle = 'Twitter status (tweet) counts aggregated using three-hour intervals',
caption = '\nSource: Data collected from Twitters REST API via rtweet'
)
rt_geo <- rt %>% filter(!is.na(lat) & !is.na(lng))
## create lat/lng variables using all available tweet and profile geo-location data
rt <- lat_lng(rt)
rt_geo <- rt %>% filter(!is.na(lat) & !is.na(lng))
leaflet(rt_geo) %>%
addTiles() %>%
addCircleMarkers(lng = ~lng
, lat = ~lat)
leaflet(rt) %>%
addTiles() %>%
addCircleMarkers(lng = ~lng
, lat = ~lat)
## plot lat and lng points onto state map
with(rt, points(lng, lat, pch = 20, cex = .75, col = rgb(0, .3, .7, .75)))
maps::map("state", lwd = .25)
## plot lat and lng points onto state map
with(rt, points(lng, lat, pch = 20, cex = .75, col = rgb(0, .3, .7, .75)))
